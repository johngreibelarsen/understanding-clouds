{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**this kernel was forked from here : https://www.kaggle.com/samusram/cloud-classifier-for-post-processing?scriptVersionId=20265194\n",
    "\n",
    "i got 0.657 using densenet201(in version 3) **\n",
    "\n",
    "i tried  efficientnetb4 but it failed for large image size,so i will try efficientnetb3 now with bce dice loss\n",
    "if you find this kernel useful,please upvote,your upvote motivates kagglers like us to share things publicly,thanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version 13**\n",
    "\n",
    "- updating previously used submission.csv with version 3's generated submission file which achieved 0.657\n",
    "\n",
    "- threshold = 0.9\n",
    "\n",
    "- efficientnetb2\n",
    "\n",
    "- loss='categorical_crossentropy', metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Intro\n",
    "In this notebook I'd create a classifier to distinguish types of cloud formations. Using this classifier I'd check if it improves currently the best LB score from the great [public notebook by Jan](https://www.kaggle.com/jpbremer/efficient-net-b4-unet-clouds). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Plan\n",
    "1. [Libraries](#Libraries)\n",
    "2. [Data Generators](#Data-Generators)\n",
    "  * [One-hot encoding classes](#One-hot-encoding-classes)\n",
    "  * [Stratified split into train/val](#Stratified-split-into-train/val)\n",
    "  * [Generator class](#Generator-class)\n",
    "3. [PR-AUC-based Callback](#PR-AUC-based-Callback)\n",
    "4. [Classifier](#Classifier)\n",
    "  * [Defining a model](#Defining-a-model)\n",
    "  * [Initial tuning of the added fully-connected layer](#Initial-tuning-of-the-added-fully-connected-layer)\n",
    "  * [Fine-tuning the whole model](#Fine-tuning-the-whole-model)\n",
    "  * [Visualizing train and val PR AUC](#Visualizing-train-and-val-PR-AUC)\n",
    "5. [Selecting postprocessing thresholds](#Selecting-postprocessing-thresholds)\n",
    "6. [Post-processing segmentation submission](#Post-processing-segmentation-submission)\n",
    "7. [Future work](#Future-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "from keras.applications.densenet import DenseNet201\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import Sequence\n",
    "from albumentations import Compose, VerticalFlip, HorizontalFlip, Rotate, GridDistortion\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from numpy.random import seed\n",
    "seed(10)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(10)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-rectified-adam\n",
      "  Downloading https://files.pythonhosted.org/packages/21/79/9521f66b92186702cb58a214c1b923b416266381cd824e15a1733f6a5b06/keras-rectified-adam-0.17.0.tar.gz\n",
      "Requirement already satisfied: numpy in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from keras-rectified-adam) (1.16.3)\n",
      "Requirement already satisfied: Keras in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from keras-rectified-adam) (2.2.4)\n",
      "Requirement already satisfied: scipy>=0.14 in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from Keras->keras-rectified-adam) (1.2.1)\n",
      "Requirement already satisfied: six>=1.9.0 in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from Keras->keras-rectified-adam) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from Keras->keras-rectified-adam) (5.1)\n",
      "Requirement already satisfied: h5py in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from Keras->keras-rectified-adam) (2.9.0)\n",
      "Requirement already satisfied: keras_applications>=1.0.6 in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from Keras->keras-rectified-adam) (1.0.7)\n",
      "Requirement already satisfied: keras_preprocessing>=1.0.5 in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from Keras->keras-rectified-adam) (1.0.9)\n",
      "Building wheels for collected packages: keras-rectified-adam\n",
      "  Building wheel for keras-rectified-adam (setup.py): started\n",
      "  Building wheel for keras-rectified-adam (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Einstein\\AppData\\Local\\pip\\Cache\\wheels\\7b\\01\\27\\3a934e1a5644f5b93c720422a6ef97034ea78a21ba71cfb549\n",
      "Successfully built keras-rectified-adam\n",
      "Installing collected packages: keras-rectified-adam\n",
      "Successfully installed keras-rectified-adam-0.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-rectified-adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "test_imgs_folder = './input/test_images/'\n",
    "train_imgs_folder = './input/train_images/'\n",
    "num_cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Label</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0011165.jpg_Fish</td>\n",
       "      <td>264918 937 266318 937 267718 937 269118 937 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0011165.jpg_Flower</td>\n",
       "      <td>1355565 1002 1356965 1002 1358365 1002 1359765...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0011165.jpg_Gravel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011165.jpg_Sugar</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002be4f.jpg_Fish</td>\n",
       "      <td>233813 878 235213 878 236613 878 238010 881 23...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image_Label                                      EncodedPixels\n",
       "0    0011165.jpg_Fish  264918 937 266318 937 267718 937 269118 937 27...\n",
       "1  0011165.jpg_Flower  1355565 1002 1356965 1002 1358365 1002 1359765...\n",
       "2  0011165.jpg_Gravel                                                NaN\n",
       "3   0011165.jpg_Sugar                                                NaN\n",
       "4    002be4f.jpg_Fish  233813 878 235213 878 236613 878 238010 881 23..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./input/train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Class</th>\n",
       "      <th>Fish</th>\n",
       "      <th>Flower</th>\n",
       "      <th>Sugar</th>\n",
       "      <th>Gravel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>{Fish, Flower}</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002be4f.jpg</td>\n",
       "      <td>{Fish, Sugar, Flower}</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0031ae9.jpg</td>\n",
       "      <td>{Fish, Sugar, Flower}</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0035239.jpg</td>\n",
       "      <td>{Gravel, Flower}</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003994e.jpg</td>\n",
       "      <td>{Gravel, Fish, Sugar}</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Image                  Class  Fish  Flower  Sugar  Gravel\n",
       "0  0011165.jpg         {Fish, Flower}     1       1      0       0\n",
       "1  002be4f.jpg  {Fish, Sugar, Flower}     1       1      1       0\n",
       "2  0031ae9.jpg  {Fish, Sugar, Flower}     1       1      1       0\n",
       "3  0035239.jpg       {Gravel, Flower}     0       1      0       1\n",
       "4  003994e.jpg  {Gravel, Fish, Sugar}     1       0      1       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df[~train_df['EncodedPixels'].isnull()]\n",
    "train_df['Image'] = train_df['Image_Label'].map(lambda x: x.split('_')[0])\n",
    "train_df['Class'] = train_df['Image_Label'].map(lambda x: x.split('_')[1])\n",
    "classes = train_df['Class'].unique()\n",
    "train_df = train_df.groupby('Image')['Class'].agg(set).reset_index()\n",
    "for class_name in classes:\n",
    "    train_df[class_name] = train_df['Class'].map(lambda x: 1 if class_name in x else 0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for fast access to ohe vectors\n",
    "img_2_ohe_vector = {img:vec for img, vec in zip(train_df['Image'], train_df.iloc[:, 2:].values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified split into train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs, val_imgs = train_test_split(train_df['Image'].values, \n",
    "                                        test_size=0.2, \n",
    "                                        stratify=train_df['Class'].map(lambda x: str(sorted(list(x)))), # sorting present classes in lexicographical order, just to be sure\n",
    "                                        random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenenerator(Sequence):\n",
    "    def __init__(self, images_list=None, folder_imgs=train_imgs_folder, \n",
    "                 batch_size=16, shuffle=True, augmentation=None,\n",
    "                 resized_height=260, resized_width=260, num_channels=3):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augmentation = augmentation\n",
    "        if images_list is None:\n",
    "            self.images_list = os.listdir(folder_imgs)\n",
    "        else:\n",
    "            self.images_list = deepcopy(images_list)\n",
    "        self.folder_imgs = folder_imgs\n",
    "        self.len = len(self.images_list) // self.batch_size\n",
    "        self.resized_height = resized_height\n",
    "        self.resized_width = resized_width\n",
    "        self.num_channels = num_channels\n",
    "        self.num_classes = 4\n",
    "        self.is_test = not 'train' in folder_imgs\n",
    "        if not shuffle and not self.is_test:\n",
    "            self.labels = [img_2_ohe_vector[img] for img in self.images_list[:self.len*self.batch_size]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def on_epoch_start(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.images_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_batch = self.images_list[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "        X = np.empty((self.batch_size, self.resized_height, self.resized_width, self.num_channels))\n",
    "        y = np.empty((self.batch_size, self.num_classes))\n",
    "\n",
    "        for i, image_name in enumerate(current_batch):\n",
    "            path = os.path.join(self.folder_imgs, image_name)\n",
    "            img = cv2.resize(cv2.imread(path), (self.resized_height, self.resized_width), interpolation=cv2.INTER_AREA).astype(np.float32)\n",
    "            if not self.augmentation is None:\n",
    "                augmented = self.augmentation(image=img)\n",
    "                img = augmented['image']\n",
    "            X[i, :, :, :] = img/255.0\n",
    "            if not self.is_test:\n",
    "                y[i, :] = img_2_ohe_vector[image_name]\n",
    "        return X, y\n",
    "\n",
    "    def get_labels(self):\n",
    "        if self.shuffle:\n",
    "            images_current = self.images_list[:self.len*self.batch_size]\n",
    "            labels = [img_2_ohe_vector[img] for img in images_current]\n",
    "        else:\n",
    "            labels = self.labels\n",
    "        return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "albumentations_train = Compose([\n",
    "    VerticalFlip(), HorizontalFlip(), Rotate(limit=20), GridDistortion()\n",
    "], p=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator_train = DataGenenerator(train_imgs, augmentation=albumentations_train)\n",
    "data_generator_train_eval = DataGenenerator(train_imgs, shuffle=False)\n",
    "data_generator_val = DataGenenerator(val_imgs, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PR-AUC-based Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The callback would be used:\n",
    "1. to estimate AUC under precision recall curve for each class,\n",
    "2. to early stop after 5 epochs of no improvement in mean PR AUC,\n",
    "3. save a model with the best PR AUC in validation,\n",
    "4. to reduce learning rate on PR AUC plateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrAucCallback(Callback):\n",
    "    def __init__(self, data_generator, num_workers=num_cores, \n",
    "                 early_stopping_patience=5, \n",
    "                 plateau_patience=3, reduction_rate=0.5,\n",
    "                 stage='train', checkpoints_path='checkpoints/'):\n",
    "        super(Callback, self).__init__()\n",
    "        self.data_generator = data_generator\n",
    "        self.num_workers = num_workers\n",
    "        self.class_names = ['Fish', 'Flower', 'Sugar', 'Gravel']\n",
    "        self.history = [[] for _ in range(len(self.class_names) + 1)] # to store per each class and also mean PR AUC\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.plateau_patience = plateau_patience\n",
    "        self.reduction_rate = reduction_rate\n",
    "        self.stage = stage\n",
    "        self.best_pr_auc = -float('inf')\n",
    "        if not os.path.exists(checkpoints_path):\n",
    "            os.makedirs(checkpoints_path)\n",
    "        self.checkpoints_path = checkpoints_path\n",
    "        \n",
    "    def compute_pr_auc(self, y_true, y_pred):\n",
    "        pr_auc_mean = 0\n",
    "        print(f\"\\n{'#'*30}\\n\")\n",
    "        for class_i in range(len(self.class_names)):\n",
    "            precision, recall, _ = precision_recall_curve(y_true[:, class_i], y_pred[:, class_i])\n",
    "            pr_auc = auc(recall, precision)\n",
    "            pr_auc_mean += pr_auc/len(self.class_names)\n",
    "            print(f\"PR AUC {self.class_names[class_i]}, {self.stage}: {pr_auc:.3f}\\n\")\n",
    "            self.history[class_i].append(pr_auc)        \n",
    "        print(f\"\\n{'#'*20}\\n PR AUC mean, {self.stage}: {pr_auc_mean:.3f}\\n{'#'*20}\\n\")\n",
    "        self.history[-1].append(pr_auc_mean)\n",
    "        return pr_auc_mean\n",
    "              \n",
    "    def is_patience_lost(self, patience):\n",
    "        if len(self.history[-1]) > patience:\n",
    "            best_performance = max(self.history[-1][-(patience + 1):-1])\n",
    "            return best_performance == self.history[-1][-(patience + 1)] and best_performance >= self.history[-1][-1]    \n",
    "              \n",
    "    def early_stopping_check(self, pr_auc_mean):\n",
    "        if self.is_patience_lost(self.early_stopping_patience):\n",
    "            self.model.stop_training = True    \n",
    "              \n",
    "    def model_checkpoint(self, pr_auc_mean, epoch):\n",
    "        if pr_auc_mean > self.best_pr_auc:\n",
    "            # remove previous checkpoints to save space\n",
    "            for checkpoint in glob.glob(os.path.join(self.checkpoints_path, 'efficientNetB3_260x260_epoch_*')):\n",
    "                os.remove(checkpoint)\n",
    "            self.best_pr_auc = pr_auc_mean\n",
    "            self.model.save(os.path.join(self.checkpoints_path, f'efficientNetB3_260x260_epoch_{epoch}_val_pr_auc_{pr_auc_mean}.h5'))              \n",
    "            print(f\"\\n{'#'*20}\\nSaved new checkpoint\\n{'#'*20}\\n\")\n",
    "              \n",
    "    def reduce_lr_on_plateau(self):\n",
    "        if self.is_patience_lost(self.plateau_patience):\n",
    "            new_lr = float(keras.backend.get_value(self.model.optimizer.lr)) * self.reduction_rate\n",
    "            keras.backend.set_value(self.model.optimizer.lr, new_lr)\n",
    "            print(f\"\\n{'#'*20}\\nReduced learning rate to {new_lr}.\\n{'#'*20}\\n\")\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict_generator(self.data_generator, workers=self.num_workers)\n",
    "        y_true = self.data_generator.get_labels()\n",
    "        # estimate AUC under precision recall curve for each class\n",
    "        pr_auc_mean = self.compute_pr_auc(y_true, y_pred)\n",
    "              \n",
    "        if self.stage == 'val':\n",
    "            # early stop after early_stopping_patience=4 epochs of no improvement in mean PR AUC\n",
    "            self.early_stopping_check(pr_auc_mean)\n",
    "\n",
    "            # save a model with the best PR AUC in validation\n",
    "            self.model_checkpoint(pr_auc_mean, epoch)\n",
    "\n",
    "            # reduce learning rate on PR AUC plateau\n",
    "            self.reduce_lr_on_plateau()            \n",
    "        \n",
    "    def get_pr_auc_history(self):\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric_callback = PrAucCallback(data_generator_train_eval)\n",
    "val_callback = PrAucCallback(data_generator_val, stage='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "def dice_coef(y_true, y_pred, smooth=0.0001):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 0.0001\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/qubvel/efficientnet\n",
      "  Cloning https://github.com/qubvel/efficientnet to c:\\users\\einstein\\appdata\\local\\temp\\pip-req-build-am20l296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Error [WinError 2] The system cannot find the file specified while executing command git clone -q https://github.com/qubvel/efficientnet C:\\Users\\Einstein\\AppData\\Local\\Temp\\pip-req-build-am20l296\n",
      "Cannot find command 'git' - do you have 'git' installed and in your PATH?\n"
     ]
    }
   ],
   "source": [
    "DELETE !pip install -U git+https://github.com/qubvel/efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet\n",
      "  Downloading https://files.pythonhosted.org/packages/97/82/f3ae07316f0461417dc54affab6e86ab188a5a22f33176d35271628b96e0/efficientnet-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: scikit-image in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from efficientnet) (0.15.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications<=1.0.8,>=1.0.7 in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from efficientnet) (1.0.7)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from scikit-image->efficientnet) (5.4.1)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from scikit-image->efficientnet) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: networkx>=2.0 in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from scikit-image->efficientnet) (2.2)\n",
      "Requirement already satisfied, skipping upgrade: imageio>=2.0.1 in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from scikit-image->efficientnet) (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.16.3)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from networkx>=2.0->scikit-image->efficientnet) (4.4.0)\n",
      "Requirement already satisfied, skipping upgrade: six in e:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.12.0)\n",
      "Installing collected packages: efficientnet\n",
      "Successfully installed efficientnet-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U --pre efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\function.py:1007: calling Graph.create_op (from tensorflow.python.framework.ops) with compute_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Shapes are always computed; don't use the compute_shapes as it has no effect.\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b3_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "44113920/44107200 [==============================] - ETA: 4: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 52s - ETA: 49 - ETA: 44 - ETA: 44 - ETA: 41 - ETA: 41 - ETA: 39 - ETA: 38 - ETA: 39 - ETA: 37 - ETA: 37 - ETA: 39 - ETA: 35 - ETA: 34 - ETA: 34 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 32 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 18s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import efficientnet.keras as efn \n",
    "def get_model():\n",
    "    K.clear_session()\n",
    "    base_model =  efn.EfficientNetB3(weights='imagenet', include_top=False, pooling='avg', input_shape=(260, 260, 3))\n",
    "    x = base_model.output\n",
    "    y_pred = Dense(4, activation='sigmoid')(x)\n",
    "    return Model(inputs=base_model.input, outputs=y_pred)\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_radam import RAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial tuning of the added fully-connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "138/138 [==============================] - ETA: 35:40 - loss: 3.1071 - acc: 0.31 - ETA: 18:00 - loss: 3.0224 - acc: 0.31 - ETA: 12:07 - loss: 2.9138 - acc: 0.30 - ETA: 9:10 - loss: 2.8477 - acc: 0.2812 - ETA: 7:24 - loss: 2.8591 - acc: 0.287 - ETA: 6:13 - loss: 2.8263 - acc: 0.296 - ETA: 5:23 - loss: 2.8084 - acc: 0.299 - ETA: 4:45 - loss: 2.8260 - acc: 0.293 - ETA: 4:15 - loss: 2.7812 - acc: 0.312 - ETA: 3:52 - loss: 2.7937 - acc: 0.318 - ETA: 3:33 - loss: 2.7771 - acc: 0.329 - ETA: 3:16 - loss: 2.8028 - acc: 0.325 - ETA: 3:17 - loss: 2.7762 - acc: 0.334 - ETA: 3:06 - loss: 2.7686 - acc: 0.334 - ETA: 2:56 - loss: 2.7438 - acc: 0.337 - ETA: 2:46 - loss: 2.7444 - acc: 0.326 - ETA: 2:42 - loss: 2.7556 - acc: 0.323 - ETA: 2:34 - loss: 2.7716 - acc: 0.322 - ETA: 2:27 - loss: 2.7752 - acc: 0.317 - ETA: 2:20 - loss: 2.7648 - acc: 0.318 - ETA: 2:25 - loss: 2.7598 - acc: 0.324 - ETA: 2:21 - loss: 2.7536 - acc: 0.325 - ETA: 2:15 - loss: 2.7582 - acc: 0.323 - ETA: 2:10 - loss: 2.7651 - acc: 0.319 - ETA: 2:05 - loss: 2.7757 - acc: 0.320 - ETA: 2:02 - loss: 2.7717 - acc: 0.320 - ETA: 1:58 - loss: 2.7721 - acc: 0.320 - ETA: 1:54 - loss: 2.7791 - acc: 0.324 - ETA: 1:59 - loss: 2.7842 - acc: 0.325 - ETA: 1:56 - loss: 2.7925 - acc: 0.325 - ETA: 1:52 - loss: 2.7994 - acc: 0.323 - ETA: 1:48 - loss: 2.7987 - acc: 0.326 - ETA: 1:45 - loss: 2.7943 - acc: 0.325 - ETA: 1:42 - loss: 2.7859 - acc: 0.327 - ETA: 1:40 - loss: 2.7759 - acc: 0.329 - ETA: 1:38 - loss: 2.7806 - acc: 0.328 - ETA: 1:42 - loss: 2.7838 - acc: 0.325 - ETA: 1:39 - loss: 2.7830 - acc: 0.323 - ETA: 1:37 - loss: 2.7840 - acc: 0.328 - ETA: 1:34 - loss: 2.7865 - acc: 0.328 - ETA: 1:31 - loss: 2.7855 - acc: 0.329 - ETA: 1:29 - loss: 2.7841 - acc: 0.330 - ETA: 1:27 - loss: 2.7884 - acc: 0.332 - ETA: 1:26 - loss: 2.7923 - acc: 0.333 - ETA: 1:27 - loss: 2.7963 - acc: 0.332 - ETA: 1:25 - loss: 2.7993 - acc: 0.333 - ETA: 1:23 - loss: 2.8140 - acc: 0.332 - ETA: 1:22 - loss: 2.8105 - acc: 0.330 - ETA: 1:20 - loss: 2.8037 - acc: 0.327 - ETA: 1:18 - loss: 2.8023 - acc: 0.326 - ETA: 1:17 - loss: 2.8005 - acc: 0.323 - ETA: 1:16 - loss: 2.8001 - acc: 0.321 - ETA: 1:16 - loss: 2.8047 - acc: 0.320 - ETA: 1:14 - loss: 2.8068 - acc: 0.318 - ETA: 1:13 - loss: 2.8054 - acc: 0.318 - ETA: 1:12 - loss: 2.8127 - acc: 0.317 - ETA: 1:10 - loss: 2.8098 - acc: 0.318 - ETA: 1:08 - loss: 2.8115 - acc: 0.317 - ETA: 1:08 - loss: 2.8184 - acc: 0.315 - ETA: 1:07 - loss: 2.8210 - acc: 0.313 - ETA: 1:06 - loss: 2.8215 - acc: 0.312 - ETA: 1:05 - loss: 2.8260 - acc: 0.308 - ETA: 1:04 - loss: 2.8213 - acc: 0.308 - ETA: 1:03 - loss: 2.8210 - acc: 0.308 - ETA: 1:01 - loss: 2.8186 - acc: 0.308 - ETA: 1:00 - loss: 2.8141 - acc: 0.308 - ETA: 1:00 - loss: 2.8157 - acc: 0.307 - ETA: 59s - loss: 2.8140 - acc: 0.307 - ETA: 58s - loss: 2.8077 - acc: 0.30 - ETA: 57s - loss: 2.8064 - acc: 0.31 - ETA: 56s - loss: 2.8046 - acc: 0.31 - ETA: 54s - loss: 2.8033 - acc: 0.31 - ETA: 53s - loss: 2.8027 - acc: 0.31 - ETA: 52s - loss: 2.8064 - acc: 0.31 - ETA: 52s - loss: 2.8091 - acc: 0.30 - ETA: 51s - loss: 2.8078 - acc: 0.31 - ETA: 50s - loss: 2.8091 - acc: 0.31 - ETA: 49s - loss: 2.8065 - acc: 0.31 - ETA: 48s - loss: 2.8085 - acc: 0.31 - ETA: 47s - loss: 2.8127 - acc: 0.30 - ETA: 46s - loss: 2.8130 - acc: 0.30 - ETA: 45s - loss: 2.8170 - acc: 0.30 - ETA: 44s - loss: 2.8176 - acc: 0.31 - ETA: 44s - loss: 2.8228 - acc: 0.30 - ETA: 43s - loss: 2.8200 - acc: 0.30 - ETA: 42s - loss: 2.8236 - acc: 0.30 - ETA: 41s - loss: 2.8209 - acc: 0.31 - ETA: 40s - loss: 2.8193 - acc: 0.30 - ETA: 39s - loss: 2.8223 - acc: 0.31 - ETA: 38s - loss: 2.8206 - acc: 0.30 - ETA: 38s - loss: 2.8259 - acc: 0.30 - ETA: 37s - loss: 2.8273 - acc: 0.30 - ETA: 36s - loss: 2.8325 - acc: 0.30 - ETA: 35s - loss: 2.8339 - acc: 0.30 - ETA: 34s - loss: 2.8357 - acc: 0.30 - ETA: 33s - loss: 2.8349 - acc: 0.30 - ETA: 32s - loss: 2.8367 - acc: 0.30 - ETA: 31s - loss: 2.8336 - acc: 0.30 - ETA: 31s - loss: 2.8321 - acc: 0.30 - ETA: 30s - loss: 2.8331 - acc: 0.30 - ETA: 29s - loss: 2.8340 - acc: 0.30 - ETA: 28s - loss: 2.8304 - acc: 0.30 - ETA: 27s - loss: 2.8290 - acc: 0.30 - ETA: 26s - loss: 2.8282 - acc: 0.30 - ETA: 26s - loss: 2.8264 - acc: 0.30 - ETA: 25s - loss: 2.8284 - acc: 0.30 - ETA: 24s - loss: 2.8245 - acc: 0.30 - ETA: 23s - loss: 2.8225 - acc: 0.30 - ETA: 22s - loss: 2.8239 - acc: 0.30 - ETA: 22s - loss: 2.8244 - acc: 0.30 - ETA: 21s - loss: 2.8208 - acc: 0.30 - ETA: 20s - loss: 2.8211 - acc: 0.30 - ETA: 19s - loss: 2.8203 - acc: 0.30 - ETA: 18s - loss: 2.8222 - acc: 0.30 - ETA: 18s - loss: 2.8217 - acc: 0.30 - ETA: 17s - loss: 2.8212 - acc: 0.30 - ETA: 16s - loss: 2.8212 - acc: 0.30 - ETA: 15s - loss: 2.8208 - acc: 0.30 - ETA: 14s - loss: 2.8212 - acc: 0.30 - ETA: 13s - loss: 2.8231 - acc: 0.30 - ETA: 13s - loss: 2.8249 - acc: 0.30 - ETA: 12s - loss: 2.8302 - acc: 0.30 - ETA: 11s - loss: 2.8281 - acc: 0.30 - ETA: 10s - loss: 2.8281 - acc: 0.30 - ETA: 10s - loss: 2.8320 - acc: 0.30 - ETA: 9s - loss: 2.8319 - acc: 0.3070 - ETA: 8s - loss: 2.8349 - acc: 0.306 - ETA: 7s - loss: 2.8368 - acc: 0.306 - ETA: 7s - loss: 2.8358 - acc: 0.306 - ETA: 6s - loss: 2.8375 - acc: 0.307 - ETA: 5s - loss: 2.8389 - acc: 0.306 - ETA: 4s - loss: 2.8411 - acc: 0.305 - ETA: 3s - loss: 2.8414 - acc: 0.306 - ETA: 3s - loss: 2.8417 - acc: 0.306 - ETA: 2s - loss: 2.8423 - acc: 0.305 - ETA: 1s - loss: 2.8443 - acc: 0.305 - ETA: 0s - loss: 2.8472 - acc: 0.305 - 129s 937ms/step - loss: 2.8502 - acc: 0.3055 - val_loss: 3.0003 - val_acc: 0.3824\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, train: 0.622\n",
      "\n",
      "PR AUC Flower, train: 0.630\n",
      "\n",
      "PR AUC Sugar, train: 0.697\n",
      "\n",
      "PR AUC Gravel, train: 0.518\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, train: 0.617\n",
      "####################\n",
      "\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, val: 0.633\n",
      "\n",
      "PR AUC Flower, val: 0.626\n",
      "\n",
      "PR AUC Sugar, val: 0.724\n",
      "\n",
      "PR AUC Gravel, val: 0.548\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, val: 0.633\n",
      "####################\n",
      "\n",
      "\n",
      "####################\n",
      "Saved new checkpoint\n",
      "####################\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 37s - loss: 2.8791 - acc: 0.31 - ETA: 37s - loss: 2.9442 - acc: 0.34 - ETA: 37s - loss: 2.9465 - acc: 0.32 - ETA: 37s - loss: 2.9042 - acc: 0.31 - ETA: 37s - loss: 2.8031 - acc: 0.36 - ETA: 37s - loss: 2.8094 - acc: 0.36 - ETA: 37s - loss: 2.8237 - acc: 0.34 - ETA: 37s - loss: 2.8494 - acc: 0.35 - ETA: 36s - loss: 2.8402 - acc: 0.35 - ETA: 36s - loss: 2.8588 - acc: 0.34 - ETA: 36s - loss: 2.8236 - acc: 0.34 - ETA: 49s - loss: 2.8410 - acc: 0.34 - ETA: 52s - loss: 2.8328 - acc: 0.34 - ETA: 54s - loss: 2.8328 - acc: 0.34 - ETA: 53s - loss: 2.8498 - acc: 0.33 - ETA: 53s - loss: 2.8419 - acc: 0.33 - ETA: 52s - loss: 2.8368 - acc: 0.33 - ETA: 52s - loss: 2.8483 - acc: 0.34 - ETA: 51s - loss: 2.8387 - acc: 0.34 - ETA: 1:01 - loss: 2.8310 - acc: 0.343 - ETA: 1:03 - loss: 2.8309 - acc: 0.337 - ETA: 1:04 - loss: 2.8222 - acc: 0.335 - ETA: 1:02 - loss: 2.8137 - acc: 0.339 - ETA: 1:02 - loss: 2.8106 - acc: 0.341 - ETA: 1:00 - loss: 2.8181 - acc: 0.338 - ETA: 59s - loss: 2.8259 - acc: 0.337 - ETA: 57s - loss: 2.8515 - acc: 0.33 - ETA: 1:02 - loss: 2.8498 - acc: 0.334 - ETA: 1:04 - loss: 2.8508 - acc: 0.333 - ETA: 1:03 - loss: 2.8346 - acc: 0.337 - ETA: 1:02 - loss: 2.8239 - acc: 0.339 - ETA: 1:01 - loss: 2.8287 - acc: 0.338 - ETA: 59s - loss: 2.8231 - acc: 0.340 - ETA: 58s - loss: 2.8128 - acc: 0.34 - ETA: 57s - loss: 2.8150 - acc: 0.34 - ETA: 1:00 - loss: 2.8142 - acc: 0.346 - ETA: 1:03 - loss: 2.8166 - acc: 0.348 - ETA: 1:01 - loss: 2.8180 - acc: 0.347 - ETA: 1:00 - loss: 2.8193 - acc: 0.351 - ETA: 59s - loss: 2.8208 - acc: 0.352 - ETA: 57s - loss: 2.8265 - acc: 0.35 - ETA: 56s - loss: 2.8183 - acc: 0.35 - ETA: 56s - loss: 2.8130 - acc: 0.34 - ETA: 57s - loss: 2.8189 - acc: 0.34 - ETA: 58s - loss: 2.8302 - acc: 0.34 - ETA: 58s - loss: 2.8356 - acc: 0.34 - ETA: 57s - loss: 2.8271 - acc: 0.34 - ETA: 56s - loss: 2.8251 - acc: 0.34 - ETA: 54s - loss: 2.8332 - acc: 0.34 - ETA: 54s - loss: 2.8293 - acc: 0.34 - ETA: 53s - loss: 2.8251 - acc: 0.34 - ETA: 53s - loss: 2.8296 - acc: 0.34 - ETA: 54s - loss: 2.8371 - acc: 0.34 - ETA: 53s - loss: 2.8438 - acc: 0.34 - ETA: 53s - loss: 2.8371 - acc: 0.34 - ETA: 52s - loss: 2.8369 - acc: 0.34 - ETA: 51s - loss: 2.8378 - acc: 0.34 - ETA: 50s - loss: 2.8414 - acc: 0.34 - ETA: 49s - loss: 2.8446 - acc: 0.33 - ETA: 49s - loss: 2.8458 - acc: 0.33 - ETA: 49s - loss: 2.8486 - acc: 0.33 - ETA: 49s - loss: 2.8473 - acc: 0.33 - ETA: 48s - loss: 2.8493 - acc: 0.33 - ETA: 47s - loss: 2.8506 - acc: 0.33 - ETA: 46s - loss: 2.8491 - acc: 0.33 - ETA: 46s - loss: 2.8490 - acc: 0.33 - ETA: 45s - loss: 2.8498 - acc: 0.33 - ETA: 44s - loss: 2.8467 - acc: 0.33 - ETA: 44s - loss: 2.8479 - acc: 0.33 - ETA: 44s - loss: 2.8524 - acc: 0.33 - ETA: 43s - loss: 2.8520 - acc: 0.33 - ETA: 42s - loss: 2.8562 - acc: 0.33 - ETA: 41s - loss: 2.8553 - acc: 0.33 - ETA: 41s - loss: 2.8571 - acc: 0.33 - ETA: 40s - loss: 2.8592 - acc: 0.33 - ETA: 39s - loss: 2.8572 - acc: 0.33 - ETA: 39s - loss: 2.8551 - acc: 0.33 - ETA: 39s - loss: 2.8564 - acc: 0.33 - ETA: 38s - loss: 2.8575 - acc: 0.33 - ETA: 37s - loss: 2.8580 - acc: 0.33 - ETA: 37s - loss: 2.8547 - acc: 0.33 - ETA: 36s - loss: 2.8573 - acc: 0.33 - ETA: 35s - loss: 2.8570 - acc: 0.33 - ETA: 35s - loss: 2.8512 - acc: 0.33 - ETA: 34s - loss: 2.8488 - acc: 0.33 - ETA: 34s - loss: 2.8478 - acc: 0.33 - ETA: 33s - loss: 2.8460 - acc: 0.34 - ETA: 32s - loss: 2.8453 - acc: 0.33 - ETA: 32s - loss: 2.8428 - acc: 0.33 - ETA: 31s - loss: 2.8426 - acc: 0.33 - ETA: 30s - loss: 2.8425 - acc: 0.33 - ETA: 30s - loss: 2.8472 - acc: 0.33 - ETA: 29s - loss: 2.8472 - acc: 0.33 - ETA: 29s - loss: 2.8473 - acc: 0.33 - ETA: 28s - loss: 2.8422 - acc: 0.33 - ETA: 27s - loss: 2.8391 - acc: 0.34 - ETA: 26s - loss: 2.8369 - acc: 0.33 - ETA: 26s - loss: 2.8339 - acc: 0.33 - ETA: 25s - loss: 2.8399 - acc: 0.33 - ETA: 25s - loss: 2.8419 - acc: 0.33 - ETA: 24s - loss: 2.8393 - acc: 0.33 - ETA: 24s - loss: 2.8371 - acc: 0.33 - ETA: 23s - loss: 2.8353 - acc: 0.33 - ETA: 22s - loss: 2.8309 - acc: 0.33 - ETA: 21s - loss: 2.8332 - acc: 0.33 - ETA: 21s - loss: 2.8349 - acc: 0.33 - ETA: 20s - loss: 2.8365 - acc: 0.33 - ETA: 19s - loss: 2.8346 - acc: 0.33 - ETA: 19s - loss: 2.8387 - acc: 0.33 - ETA: 18s - loss: 2.8369 - acc: 0.33 - ETA: 18s - loss: 2.8385 - acc: 0.33 - ETA: 17s - loss: 2.8370 - acc: 0.33 - ETA: 16s - loss: 2.8379 - acc: 0.33 - ETA: 16s - loss: 2.8339 - acc: 0.33 - ETA: 15s - loss: 2.8378 - acc: 0.33 - ETA: 14s - loss: 2.8381 - acc: 0.33 - ETA: 14s - loss: 2.8355 - acc: 0.33 - ETA: 13s - loss: 2.8348 - acc: 0.33 - ETA: 12s - loss: 2.8336 - acc: 0.33 - ETA: 12s - loss: 2.8359 - acc: 0.33 - ETA: 11s - loss: 2.8326 - acc: 0.32 - ETA: 10s - loss: 2.8338 - acc: 0.32 - ETA: 10s - loss: 2.8358 - acc: 0.32 - ETA: 9s - loss: 2.8352 - acc: 0.3284 - ETA: 8s - loss: 2.8347 - acc: 0.328 - ETA: 8s - loss: 2.8332 - acc: 0.328 - ETA: 7s - loss: 2.8336 - acc: 0.329 - ETA: 6s - loss: 2.8321 - acc: 0.329 - ETA: 6s - loss: 2.8317 - acc: 0.329 - ETA: 5s - loss: 2.8348 - acc: 0.328 - ETA: 4s - loss: 2.8327 - acc: 0.329 - ETA: 4s - loss: 2.8327 - acc: 0.330 - ETA: 3s - loss: 2.8308 - acc: 0.330 - ETA: 2s - loss: 2.8317 - acc: 0.330 - ETA: 2s - loss: 2.8311 - acc: 0.330 - ETA: 1s - loss: 2.8312 - acc: 0.330 - ETA: 0s - loss: 2.8308 - acc: 0.330 - 115s 837ms/step - loss: 2.8309 - acc: 0.3304 - val_loss: 2.9889 - val_acc: 0.5018\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, train: 0.625\n",
      "\n",
      "PR AUC Flower, train: 0.638\n",
      "\n",
      "PR AUC Sugar, train: 0.707\n",
      "\n",
      "PR AUC Gravel, train: 0.521\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, train: 0.623\n",
      "####################\n",
      "\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, val: 0.636\n",
      "\n",
      "PR AUC Flower, val: 0.638\n",
      "\n",
      "PR AUC Sugar, val: 0.731\n",
      "\n",
      "PR AUC Gravel, val: 0.558\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, val: 0.641\n",
      "####################\n",
      "\n",
      "\n",
      "####################\n",
      "Saved new checkpoint\n",
      "####################\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 40s - loss: 2.8489 - acc: 0.21 - ETA: 42s - loss: 2.7623 - acc: 0.28 - ETA: 43s - loss: 2.8708 - acc: 0.22 - ETA: 43s - loss: 2.8539 - acc: 0.26 - ETA: 44s - loss: 2.8717 - acc: 0.28 - ETA: 45s - loss: 2.8640 - acc: 0.32 - ETA: 46s - loss: 2.8772 - acc: 0.33 - ETA: 45s - loss: 2.9185 - acc: 0.33 - ETA: 45s - loss: 2.9086 - acc: 0.35 - ETA: 45s - loss: 2.8860 - acc: 0.36 - ETA: 45s - loss: 2.8635 - acc: 0.36 - ETA: 1:19 - loss: 2.8390 - acc: 0.369 - ETA: 1:23 - loss: 2.8217 - acc: 0.363 - ETA: 1:20 - loss: 2.8595 - acc: 0.359 - ETA: 1:17 - loss: 2.8607 - acc: 0.368 - ETA: 1:14 - loss: 2.8612 - acc: 0.373 - ETA: 1:11 - loss: 2.8597 - acc: 0.364 - ETA: 1:09 - loss: 2.8382 - acc: 0.362 - ETA: 1:06 - loss: 2.8283 - acc: 0.361 - ETA: 1:15 - loss: 2.8159 - acc: 0.365 - ETA: 1:16 - loss: 2.8202 - acc: 0.363 - ETA: 1:15 - loss: 2.8285 - acc: 0.363 - ETA: 1:13 - loss: 2.8373 - acc: 0.357 - ETA: 1:11 - loss: 2.8294 - acc: 0.360 - ETA: 1:09 - loss: 2.8447 - acc: 0.357 - ETA: 1:07 - loss: 2.8454 - acc: 0.353 - ETA: 1:05 - loss: 2.8417 - acc: 0.347 - ETA: 1:11 - loss: 2.8508 - acc: 0.346 - ETA: 1:11 - loss: 2.8476 - acc: 0.345 - ETA: 1:10 - loss: 2.8480 - acc: 0.349 - ETA: 1:09 - loss: 2.8526 - acc: 0.348 - ETA: 1:07 - loss: 2.8357 - acc: 0.353 - ETA: 1:06 - loss: 2.8274 - acc: 0.354 - ETA: 1:04 - loss: 2.8362 - acc: 0.353 - ETA: 1:03 - loss: 2.8395 - acc: 0.354 - ETA: 1:06 - loss: 2.8340 - acc: 0.355 - ETA: 1:05 - loss: 2.8250 - acc: 0.355 - ETA: 1:06 - loss: 2.8134 - acc: 0.359 - ETA: 1:04 - loss: 2.8182 - acc: 0.359 - ETA: 1:03 - loss: 2.8128 - acc: 0.363 - ETA: 1:02 - loss: 2.8179 - acc: 0.359 - ETA: 1:02 - loss: 2.8200 - acc: 0.362 - ETA: 1:00 - loss: 2.8177 - acc: 0.364 - ETA: 1:01 - loss: 2.8218 - acc: 0.364 - ETA: 1:01 - loss: 2.8246 - acc: 0.363 - ETA: 1:00 - loss: 2.8317 - acc: 0.362 - ETA: 59s - loss: 2.8271 - acc: 0.363 - ETA: 59s - loss: 2.8283 - acc: 0.36 - ETA: 58s - loss: 2.8319 - acc: 0.36 - ETA: 57s - loss: 2.8234 - acc: 0.36 - ETA: 56s - loss: 2.8160 - acc: 0.36 - ETA: 56s - loss: 2.8092 - acc: 0.36 - ETA: 56s - loss: 2.8005 - acc: 0.36 - ETA: 55s - loss: 2.7985 - acc: 0.36 - ETA: 54s - loss: 2.8087 - acc: 0.36 - ETA: 54s - loss: 2.8107 - acc: 0.36 - ETA: 54s - loss: 2.8120 - acc: 0.36 - ETA: 53s - loss: 2.8088 - acc: 0.36 - ETA: 52s - loss: 2.8086 - acc: 0.36 - ETA: 51s - loss: 2.8049 - acc: 0.35 - ETA: 51s - loss: 2.8042 - acc: 0.36 - ETA: 50s - loss: 2.8049 - acc: 0.35 - ETA: 49s - loss: 2.8113 - acc: 0.36 - ETA: 49s - loss: 2.8136 - acc: 0.35 - ETA: 49s - loss: 2.8068 - acc: 0.35 - ETA: 48s - loss: 2.8099 - acc: 0.35 - ETA: 47s - loss: 2.8193 - acc: 0.36 - ETA: 46s - loss: 2.8189 - acc: 0.36 - ETA: 46s - loss: 2.8164 - acc: 0.36 - ETA: 45s - loss: 2.8136 - acc: 0.36 - ETA: 44s - loss: 2.8108 - acc: 0.36 - ETA: 44s - loss: 2.8048 - acc: 0.36 - ETA: 44s - loss: 2.8076 - acc: 0.36 - ETA: 43s - loss: 2.8122 - acc: 0.36 - ETA: 42s - loss: 2.8069 - acc: 0.36 - ETA: 41s - loss: 2.8038 - acc: 0.36 - ETA: 40s - loss: 2.8003 - acc: 0.36 - ETA: 40s - loss: 2.8023 - acc: 0.36 - ETA: 39s - loss: 2.8044 - acc: 0.36 - ETA: 39s - loss: 2.8044 - acc: 0.35 - ETA: 39s - loss: 2.8027 - acc: 0.35 - ETA: 38s - loss: 2.8032 - acc: 0.35 - ETA: 37s - loss: 2.8001 - acc: 0.35 - ETA: 36s - loss: 2.7998 - acc: 0.35 - ETA: 35s - loss: 2.7998 - acc: 0.36 - ETA: 34s - loss: 2.8029 - acc: 0.36 - ETA: 34s - loss: 2.8024 - acc: 0.36 - ETA: 34s - loss: 2.7987 - acc: 0.36 - ETA: 33s - loss: 2.8006 - acc: 0.36 - ETA: 32s - loss: 2.8009 - acc: 0.36 - ETA: 31s - loss: 2.8005 - acc: 0.36 - ETA: 30s - loss: 2.8019 - acc: 0.36 - ETA: 30s - loss: 2.8011 - acc: 0.36 - ETA: 29s - loss: 2.8043 - acc: 0.35 - ETA: 28s - loss: 2.8060 - acc: 0.35 - ETA: 28s - loss: 2.8044 - acc: 0.35 - ETA: 28s - loss: 2.8040 - acc: 0.35 - ETA: 27s - loss: 2.8088 - acc: 0.35 - ETA: 26s - loss: 2.8116 - acc: 0.36 - ETA: 25s - loss: 2.8072 - acc: 0.36 - ETA: 24s - loss: 2.8090 - acc: 0.36 - ETA: 24s - loss: 2.8125 - acc: 0.36 - ETA: 23s - loss: 2.8104 - acc: 0.36 - ETA: 23s - loss: 2.8116 - acc: 0.36 - ETA: 22s - loss: 2.8124 - acc: 0.36 - ETA: 22s - loss: 2.8085 - acc: 0.36 - ETA: 21s - loss: 2.8058 - acc: 0.36 - ETA: 20s - loss: 2.8086 - acc: 0.36 - ETA: 19s - loss: 2.8070 - acc: 0.36 - ETA: 18s - loss: 2.8060 - acc: 0.36 - ETA: 18s - loss: 2.8055 - acc: 0.36 - ETA: 17s - loss: 2.8051 - acc: 0.36 - ETA: 17s - loss: 2.8049 - acc: 0.36 - ETA: 16s - loss: 2.8039 - acc: 0.36 - ETA: 15s - loss: 2.8059 - acc: 0.36 - ETA: 15s - loss: 2.8074 - acc: 0.36 - ETA: 14s - loss: 2.8057 - acc: 0.35 - ETA: 13s - loss: 2.8056 - acc: 0.36 - ETA: 12s - loss: 2.8038 - acc: 0.36 - ETA: 12s - loss: 2.8019 - acc: 0.36 - ETA: 11s - loss: 2.8006 - acc: 0.36 - ETA: 11s - loss: 2.8060 - acc: 0.36 - ETA: 10s - loss: 2.8066 - acc: 0.36 - ETA: 9s - loss: 2.8050 - acc: 0.3614 - ETA: 8s - loss: 2.8051 - acc: 0.361 - ETA: 8s - loss: 2.8056 - acc: 0.361 - ETA: 7s - loss: 2.8068 - acc: 0.361 - ETA: 6s - loss: 2.8084 - acc: 0.362 - ETA: 6s - loss: 2.8076 - acc: 0.362 - ETA: 5s - loss: 2.8071 - acc: 0.363 - ETA: 4s - loss: 2.8071 - acc: 0.363 - ETA: 4s - loss: 2.8083 - acc: 0.362 - ETA: 3s - loss: 2.8096 - acc: 0.362 - ETA: 2s - loss: 2.8122 - acc: 0.361 - ETA: 2s - loss: 2.8125 - acc: 0.361 - ETA: 1s - loss: 2.8129 - acc: 0.361 - ETA: 0s - loss: 2.8141 - acc: 0.361 - 119s 865ms/step - loss: 2.8141 - acc: 0.3616 - val_loss: 2.9440 - val_acc: 0.4779\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, train: 0.630\n",
      "\n",
      "PR AUC Flower, train: 0.636\n",
      "\n",
      "PR AUC Sugar, train: 0.710\n",
      "\n",
      "PR AUC Gravel, train: 0.536\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, train: 0.628\n",
      "####################\n",
      "\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, val: 0.640\n",
      "\n",
      "PR AUC Flower, val: 0.637\n",
      "\n",
      "PR AUC Sugar, val: 0.733\n",
      "\n",
      "PR AUC Gravel, val: 0.577\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, val: 0.647\n",
      "####################\n",
      "\n",
      "\n",
      "####################\n",
      "Saved new checkpoint\n",
      "####################\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 39s - loss: 2.8700 - acc: 0.46 - ETA: 42s - loss: 2.7872 - acc: 0.43 - ETA: 42s - loss: 2.8020 - acc: 0.44 - ETA: 43s - loss: 2.8046 - acc: 0.42 - ETA: 43s - loss: 2.7779 - acc: 0.40 - ETA: 43s - loss: 2.7560 - acc: 0.42 - ETA: 44s - loss: 2.7825 - acc: 0.40 - ETA: 44s - loss: 2.7970 - acc: 0.40 - ETA: 44s - loss: 2.8276 - acc: 0.40 - ETA: 43s - loss: 2.7818 - acc: 0.41 - ETA: 43s - loss: 2.8225 - acc: 0.40 - ETA: 1:33 - loss: 2.8245 - acc: 0.393 - ETA: 1:28 - loss: 2.8317 - acc: 0.384 - ETA: 1:24 - loss: 2.8466 - acc: 0.370 - ETA: 1:20 - loss: 2.8376 - acc: 0.370 - ETA: 1:16 - loss: 2.8301 - acc: 0.371 - ETA: 1:14 - loss: 2.8164 - acc: 0.371 - ETA: 1:11 - loss: 2.8206 - acc: 0.362 - ETA: 1:09 - loss: 2.8087 - acc: 0.366 - ETA: 1:23 - loss: 2.8206 - acc: 0.368 - ETA: 1:22 - loss: 2.8304 - acc: 0.363 - ETA: 1:19 - loss: 2.8373 - acc: 0.365 - ETA: 1:17 - loss: 2.8459 - acc: 0.360 - ETA: 1:14 - loss: 2.8445 - acc: 0.363 - ETA: 1:12 - loss: 2.8470 - acc: 0.358 - ETA: 1:10 - loss: 2.8422 - acc: 0.358 - ETA: 1:08 - loss: 2.8435 - acc: 0.356 - ETA: 1:17 - loss: 2.8471 - acc: 0.353 - ETA: 1:16 - loss: 2.8453 - acc: 0.358 - ETA: 1:14 - loss: 2.8320 - acc: 0.362 - ETA: 1:12 - loss: 2.8250 - acc: 0.362 - ETA: 1:10 - loss: 2.8091 - acc: 0.365 - ETA: 1:08 - loss: 2.7972 - acc: 0.363 - ETA: 1:06 - loss: 2.7962 - acc: 0.364 - ETA: 1:05 - loss: 2.8032 - acc: 0.360 - ETA: 1:11 - loss: 2.8062 - acc: 0.356 - ETA: 1:10 - loss: 2.8022 - acc: 0.356 - ETA: 1:08 - loss: 2.7971 - acc: 0.361 - ETA: 1:07 - loss: 2.7932 - acc: 0.363 - ETA: 1:05 - loss: 2.7970 - acc: 0.363 - ETA: 1:04 - loss: 2.7947 - acc: 0.364 - ETA: 1:02 - loss: 2.8059 - acc: 0.362 - ETA: 1:01 - loss: 2.8030 - acc: 0.362 - ETA: 1:05 - loss: 2.8035 - acc: 0.363 - ETA: 1:04 - loss: 2.7962 - acc: 0.363 - ETA: 1:03 - loss: 2.7993 - acc: 0.360 - ETA: 1:02 - loss: 2.7957 - acc: 0.361 - ETA: 1:00 - loss: 2.7941 - acc: 0.362 - ETA: 59s - loss: 2.7900 - acc: 0.362 - ETA: 58s - loss: 2.7892 - acc: 0.36 - ETA: 56s - loss: 2.7917 - acc: 0.36 - ETA: 59s - loss: 2.7973 - acc: 0.36 - ETA: 58s - loss: 2.7981 - acc: 0.36 - ETA: 57s - loss: 2.7988 - acc: 0.36 - ETA: 56s - loss: 2.7967 - acc: 0.36 - ETA: 55s - loss: 2.7993 - acc: 0.36 - ETA: 54s - loss: 2.7963 - acc: 0.36 - ETA: 53s - loss: 2.8019 - acc: 0.36 - ETA: 52s - loss: 2.8028 - acc: 0.35 - ETA: 54s - loss: 2.7971 - acc: 0.35 - ETA: 53s - loss: 2.7976 - acc: 0.35 - ETA: 52s - loss: 2.7899 - acc: 0.35 - ETA: 51s - loss: 2.7922 - acc: 0.35 - ETA: 50s - loss: 2.7886 - acc: 0.35 - ETA: 49s - loss: 2.7871 - acc: 0.36 - ETA: 48s - loss: 2.7835 - acc: 0.35 - ETA: 47s - loss: 2.7883 - acc: 0.35 - ETA: 48s - loss: 2.7829 - acc: 0.35 - ETA: 48s - loss: 2.7859 - acc: 0.35 - ETA: 47s - loss: 2.7856 - acc: 0.35 - ETA: 46s - loss: 2.7833 - acc: 0.36 - ETA: 45s - loss: 2.7811 - acc: 0.36 - ETA: 44s - loss: 2.7785 - acc: 0.35 - ETA: 43s - loss: 2.7769 - acc: 0.35 - ETA: 42s - loss: 2.7749 - acc: 0.35 - ETA: 43s - loss: 2.7795 - acc: 0.35 - ETA: 42s - loss: 2.7757 - acc: 0.35 - ETA: 41s - loss: 2.7734 - acc: 0.35 - ETA: 40s - loss: 2.7716 - acc: 0.35 - ETA: 39s - loss: 2.7726 - acc: 0.35 - ETA: 38s - loss: 2.7782 - acc: 0.35 - ETA: 37s - loss: 2.7777 - acc: 0.35 - ETA: 37s - loss: 2.7854 - acc: 0.35 - ETA: 37s - loss: 2.7849 - acc: 0.35 - ETA: 37s - loss: 2.7868 - acc: 0.35 - ETA: 36s - loss: 2.7886 - acc: 0.35 - ETA: 35s - loss: 2.7913 - acc: 0.35 - ETA: 34s - loss: 2.7916 - acc: 0.35 - ETA: 33s - loss: 2.7902 - acc: 0.35 - ETA: 32s - loss: 2.7880 - acc: 0.35 - ETA: 31s - loss: 2.7905 - acc: 0.35 - ETA: 32s - loss: 2.7927 - acc: 0.35 - ETA: 31s - loss: 2.7938 - acc: 0.35 - ETA: 30s - loss: 2.7967 - acc: 0.35 - ETA: 29s - loss: 2.7927 - acc: 0.35 - ETA: 28s - loss: 2.7929 - acc: 0.35 - ETA: 28s - loss: 2.7955 - acc: 0.35 - ETA: 27s - loss: 2.7938 - acc: 0.35 - ETA: 26s - loss: 2.7931 - acc: 0.35 - ETA: 26s - loss: 2.7929 - acc: 0.35 - ETA: 25s - loss: 2.7938 - acc: 0.35 - ETA: 24s - loss: 2.7964 - acc: 0.35 - ETA: 24s - loss: 2.7946 - acc: 0.35 - ETA: 23s - loss: 2.7959 - acc: 0.35 - ETA: 22s - loss: 2.7945 - acc: 0.35 - ETA: 21s - loss: 2.7929 - acc: 0.35 - ETA: 21s - loss: 2.7935 - acc: 0.36 - ETA: 20s - loss: 2.7982 - acc: 0.36 - ETA: 20s - loss: 2.7942 - acc: 0.36 - ETA: 19s - loss: 2.7933 - acc: 0.36 - ETA: 18s - loss: 2.7933 - acc: 0.36 - ETA: 18s - loss: 2.7992 - acc: 0.35 - ETA: 17s - loss: 2.7989 - acc: 0.35 - ETA: 16s - loss: 2.8002 - acc: 0.35 - ETA: 15s - loss: 2.7986 - acc: 0.35 - ETA: 15s - loss: 2.7960 - acc: 0.36 - ETA: 14s - loss: 2.7926 - acc: 0.36 - ETA: 13s - loss: 2.7945 - acc: 0.36 - ETA: 13s - loss: 2.7960 - acc: 0.36 - ETA: 12s - loss: 2.7982 - acc: 0.35 - ETA: 11s - loss: 2.7983 - acc: 0.36 - ETA: 11s - loss: 2.7990 - acc: 0.36 - ETA: 10s - loss: 2.8031 - acc: 0.36 - ETA: 9s - loss: 2.7987 - acc: 0.3616 - ETA: 9s - loss: 2.7978 - acc: 0.361 - ETA: 8s - loss: 2.8011 - acc: 0.360 - ETA: 7s - loss: 2.7976 - acc: 0.361 - ETA: 7s - loss: 2.7981 - acc: 0.361 - ETA: 6s - loss: 2.7973 - acc: 0.360 - ETA: 5s - loss: 2.7975 - acc: 0.361 - ETA: 4s - loss: 2.7941 - acc: 0.361 - ETA: 4s - loss: 2.7955 - acc: 0.360 - ETA: 3s - loss: 2.7945 - acc: 0.360 - ETA: 2s - loss: 2.7923 - acc: 0.360 - ETA: 2s - loss: 2.7926 - acc: 0.360 - ETA: 1s - loss: 2.7945 - acc: 0.360 - ETA: 0s - loss: 2.7976 - acc: 0.360 - 119s 860ms/step - loss: 2.7980 - acc: 0.3603 - val_loss: 2.8985 - val_acc: 0.4283\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, train: 0.623\n",
      "\n",
      "PR AUC Flower, train: 0.652\n",
      "\n",
      "PR AUC Sugar, train: 0.720\n",
      "\n",
      "PR AUC Gravel, train: 0.540\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, train: 0.634\n",
      "####################\n",
      "\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, val: 0.635\n",
      "\n",
      "PR AUC Flower, val: 0.660\n",
      "\n",
      "PR AUC Sugar, val: 0.739\n",
      "\n",
      "PR AUC Gravel, val: 0.579\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, val: 0.653\n",
      "####################\n",
      "\n",
      "\n",
      "####################\n",
      "Saved new checkpoint\n",
      "####################\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 41s - loss: 2.5473 - acc: 0.37 - ETA: 42s - loss: 2.7223 - acc: 0.29 - ETA: 43s - loss: 2.8137 - acc: 0.34 - ETA: 43s - loss: 2.7147 - acc: 0.33 - ETA: 43s - loss: 2.7015 - acc: 0.36 - ETA: 43s - loss: 2.7050 - acc: 0.34 - ETA: 43s - loss: 2.7456 - acc: 0.33 - ETA: 43s - loss: 2.8120 - acc: 0.32 - ETA: 43s - loss: 2.7694 - acc: 0.33 - ETA: 43s - loss: 2.7642 - acc: 0.35 - ETA: 43s - loss: 2.7640 - acc: 0.34 - ETA: 1:25 - loss: 2.7392 - acc: 0.343 - ETA: 1:27 - loss: 2.7391 - acc: 0.348 - ETA: 1:23 - loss: 2.7942 - acc: 0.343 - ETA: 1:20 - loss: 2.7949 - acc: 0.352 - ETA: 1:16 - loss: 2.7803 - acc: 0.351 - ETA: 1:14 - loss: 2.7763 - acc: 0.354 - ETA: 1:11 - loss: 2.7679 - acc: 0.352 - ETA: 1:09 - loss: 2.7563 - acc: 0.353 - ETA: 1:23 - loss: 2.7624 - acc: 0.348 - ETA: 1:27 - loss: 2.7605 - acc: 0.349 - ETA: 1:24 - loss: 2.7692 - acc: 0.350 - ETA: 1:21 - loss: 2.7699 - acc: 0.351 - ETA: 1:18 - loss: 2.7776 - acc: 0.350 - ETA: 1:16 - loss: 2.7866 - acc: 0.350 - ETA: 1:14 - loss: 2.7960 - acc: 0.347 - ETA: 1:12 - loss: 2.7919 - acc: 0.350 - ETA: 1:17 - loss: 2.7977 - acc: 0.349 - ETA: 1:17 - loss: 2.7923 - acc: 0.349 - ETA: 1:15 - loss: 2.8073 - acc: 0.347 - ETA: 1:13 - loss: 2.8103 - acc: 0.346 - ETA: 1:12 - loss: 2.8043 - acc: 0.347 - ETA: 1:10 - loss: 2.8044 - acc: 0.350 - ETA: 1:08 - loss: 2.8029 - acc: 0.350 - ETA: 1:07 - loss: 2.8008 - acc: 0.351 - ETA: 1:11 - loss: 2.7958 - acc: 0.353 - ETA: 1:11 - loss: 2.7952 - acc: 0.355 - ETA: 1:09 - loss: 2.8000 - acc: 0.351 - ETA: 1:08 - loss: 2.8067 - acc: 0.349 - ETA: 1:07 - loss: 2.7963 - acc: 0.351 - ETA: 1:06 - loss: 2.8037 - acc: 0.353 - ETA: 1:04 - loss: 2.8012 - acc: 0.356 - ETA: 1:03 - loss: 2.8066 - acc: 0.356 - ETA: 1:05 - loss: 2.7992 - acc: 0.356 - ETA: 1:05 - loss: 2.7895 - acc: 0.358 - ETA: 1:04 - loss: 2.7975 - acc: 0.357 - ETA: 1:02 - loss: 2.7882 - acc: 0.357 - ETA: 1:02 - loss: 2.7914 - acc: 0.356 - ETA: 1:01 - loss: 2.7914 - acc: 0.356 - ETA: 1:00 - loss: 2.7955 - acc: 0.355 - ETA: 58s - loss: 2.7963 - acc: 0.358 - ETA: 1:00 - loss: 2.8000 - acc: 0.359 - ETA: 1:00 - loss: 2.8019 - acc: 0.356 - ETA: 58s - loss: 2.7984 - acc: 0.357 - ETA: 57s - loss: 2.7953 - acc: 0.35 - ETA: 57s - loss: 2.7976 - acc: 0.35 - ETA: 56s - loss: 2.8024 - acc: 0.35 - ETA: 55s - loss: 2.7943 - acc: 0.36 - ETA: 54s - loss: 2.7988 - acc: 0.35 - ETA: 55s - loss: 2.7989 - acc: 0.35 - ETA: 54s - loss: 2.7992 - acc: 0.35 - ETA: 53s - loss: 2.8018 - acc: 0.35 - ETA: 52s - loss: 2.8057 - acc: 0.35 - ETA: 51s - loss: 2.8062 - acc: 0.35 - ETA: 50s - loss: 2.8041 - acc: 0.35 - ETA: 50s - loss: 2.7987 - acc: 0.35 - ETA: 49s - loss: 2.7956 - acc: 0.36 - ETA: 49s - loss: 2.7988 - acc: 0.36 - ETA: 48s - loss: 2.7967 - acc: 0.36 - ETA: 47s - loss: 2.7897 - acc: 0.36 - ETA: 46s - loss: 2.7940 - acc: 0.36 - ETA: 46s - loss: 2.7953 - acc: 0.36 - ETA: 45s - loss: 2.7923 - acc: 0.36 - ETA: 44s - loss: 2.7930 - acc: 0.36 - ETA: 43s - loss: 2.7986 - acc: 0.36 - ETA: 43s - loss: 2.7971 - acc: 0.35 - ETA: 42s - loss: 2.8008 - acc: 0.35 - ETA: 41s - loss: 2.7988 - acc: 0.35 - ETA: 40s - loss: 2.8001 - acc: 0.35 - ETA: 40s - loss: 2.8008 - acc: 0.35 - ETA: 39s - loss: 2.7993 - acc: 0.35 - ETA: 39s - loss: 2.7997 - acc: 0.35 - ETA: 38s - loss: 2.8022 - acc: 0.35 - ETA: 37s - loss: 2.7973 - acc: 0.35 - ETA: 36s - loss: 2.7986 - acc: 0.36 - ETA: 35s - loss: 2.8006 - acc: 0.36 - ETA: 35s - loss: 2.8048 - acc: 0.36 - ETA: 35s - loss: 2.8031 - acc: 0.36 - ETA: 34s - loss: 2.8005 - acc: 0.36 - ETA: 33s - loss: 2.8031 - acc: 0.36 - ETA: 32s - loss: 2.8013 - acc: 0.36 - ETA: 32s - loss: 2.8022 - acc: 0.36 - ETA: 31s - loss: 2.8010 - acc: 0.36 - ETA: 30s - loss: 2.7991 - acc: 0.36 - ETA: 29s - loss: 2.7956 - acc: 0.36 - ETA: 29s - loss: 2.7932 - acc: 0.36 - ETA: 28s - loss: 2.7923 - acc: 0.36 - ETA: 28s - loss: 2.7909 - acc: 0.36 - ETA: 27s - loss: 2.7905 - acc: 0.36 - ETA: 26s - loss: 2.7912 - acc: 0.36 - ETA: 25s - loss: 2.7962 - acc: 0.36 - ETA: 24s - loss: 2.7985 - acc: 0.36 - ETA: 24s - loss: 2.7951 - acc: 0.36 - ETA: 24s - loss: 2.7919 - acc: 0.36 - ETA: 23s - loss: 2.7912 - acc: 0.36 - ETA: 22s - loss: 2.7890 - acc: 0.36 - ETA: 21s - loss: 2.7904 - acc: 0.36 - ETA: 20s - loss: 2.7888 - acc: 0.36 - ETA: 20s - loss: 2.7893 - acc: 0.36 - ETA: 19s - loss: 2.7849 - acc: 0.36 - ETA: 18s - loss: 2.7818 - acc: 0.36 - ETA: 18s - loss: 2.7812 - acc: 0.36 - ETA: 17s - loss: 2.7869 - acc: 0.36 - ETA: 16s - loss: 2.7860 - acc: 0.36 - ETA: 16s - loss: 2.7852 - acc: 0.36 - ETA: 15s - loss: 2.7859 - acc: 0.36 - ETA: 14s - loss: 2.7897 - acc: 0.35 - ETA: 13s - loss: 2.7932 - acc: 0.36 - ETA: 13s - loss: 2.7940 - acc: 0.36 - ETA: 12s - loss: 2.7941 - acc: 0.36 - ETA: 11s - loss: 2.7952 - acc: 0.35 - ETA: 11s - loss: 2.7966 - acc: 0.35 - ETA: 10s - loss: 2.7968 - acc: 0.35 - ETA: 9s - loss: 2.7952 - acc: 0.3584 - ETA: 9s - loss: 2.7956 - acc: 0.359 - ETA: 8s - loss: 2.7963 - acc: 0.359 - ETA: 7s - loss: 2.7981 - acc: 0.357 - ETA: 7s - loss: 2.7984 - acc: 0.357 - ETA: 6s - loss: 2.7993 - acc: 0.358 - ETA: 5s - loss: 2.7995 - acc: 0.357 - ETA: 4s - loss: 2.7980 - acc: 0.358 - ETA: 4s - loss: 2.7981 - acc: 0.356 - ETA: 3s - loss: 2.8003 - acc: 0.355 - ETA: 2s - loss: 2.7982 - acc: 0.356 - ETA: 2s - loss: 2.7984 - acc: 0.356 - ETA: 1s - loss: 2.7983 - acc: 0.356 - ETA: 0s - loss: 2.7969 - acc: 0.356 - 119s 860ms/step - loss: 2.7941 - acc: 0.3567 - val_loss: 2.9358 - val_acc: 0.5414\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, train: 0.631\n",
      "\n",
      "PR AUC Flower, train: 0.670\n",
      "\n",
      "PR AUC Sugar, train: 0.716\n",
      "\n",
      "PR AUC Gravel, train: 0.538\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, train: 0.639\n",
      "####################\n",
      "\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, val: 0.638\n",
      "\n",
      "PR AUC Flower, val: 0.672\n",
      "\n",
      "PR AUC Sugar, val: 0.735\n",
      "\n",
      "PR AUC Gravel, val: 0.575\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, val: 0.655\n",
      "####################\n",
      "\n",
      "\n",
      "####################\n",
      "Saved new checkpoint\n",
      "####################\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 38s - loss: 2.6090 - acc: 0.34 - ETA: 40s - loss: 2.5785 - acc: 0.35 - ETA: 41s - loss: 2.6821 - acc: 0.34 - ETA: 42s - loss: 2.6167 - acc: 0.32 - ETA: 42s - loss: 2.6277 - acc: 0.32 - ETA: 43s - loss: 2.6086 - acc: 0.33 - ETA: 43s - loss: 2.5709 - acc: 0.34 - ETA: 44s - loss: 2.5812 - acc: 0.34 - ETA: 44s - loss: 2.6073 - acc: 0.34 - ETA: 44s - loss: 2.6145 - acc: 0.36 - ETA: 44s - loss: 2.6221 - acc: 0.37 - ETA: 1:21 - loss: 2.6564 - acc: 0.380 - ETA: 1:23 - loss: 2.6631 - acc: 0.379 - ETA: 1:19 - loss: 2.6612 - acc: 0.377 - ETA: 1:16 - loss: 2.6808 - acc: 0.381 - ETA: 1:13 - loss: 2.6957 - acc: 0.377 - ETA: 1:10 - loss: 2.6915 - acc: 0.380 - ETA: 1:08 - loss: 2.6963 - acc: 0.376 - ETA: 1:05 - loss: 2.7140 - acc: 0.378 - ETA: 1:17 - loss: 2.7145 - acc: 0.381 - ETA: 1:18 - loss: 2.7348 - acc: 0.376 - ETA: 1:16 - loss: 2.7464 - acc: 0.377 - ETA: 1:14 - loss: 2.7443 - acc: 0.380 - ETA: 1:12 - loss: 2.7650 - acc: 0.377 - ETA: 1:09 - loss: 2.7605 - acc: 0.378 - ETA: 1:07 - loss: 2.7440 - acc: 0.383 - ETA: 1:06 - loss: 2.7458 - acc: 0.384 - ETA: 1:13 - loss: 2.7402 - acc: 0.383 - ETA: 1:12 - loss: 2.7469 - acc: 0.385 - ETA: 1:11 - loss: 2.7528 - acc: 0.385 - ETA: 1:09 - loss: 2.7435 - acc: 0.377 - ETA: 1:07 - loss: 2.7412 - acc: 0.380 - ETA: 1:05 - loss: 2.7337 - acc: 0.381 - ETA: 1:04 - loss: 2.7431 - acc: 0.377 - ETA: 1:03 - loss: 2.7488 - acc: 0.375 - ETA: 1:08 - loss: 2.7444 - acc: 0.374 - ETA: 1:07 - loss: 2.7497 - acc: 0.371 - ETA: 1:06 - loss: 2.7510 - acc: 0.369 - ETA: 1:05 - loss: 2.7552 - acc: 0.366 - ETA: 1:04 - loss: 2.7538 - acc: 0.365 - ETA: 1:02 - loss: 2.7568 - acc: 0.365 - ETA: 1:01 - loss: 2.7550 - acc: 0.369 - ETA: 1:00 - loss: 2.7563 - acc: 0.372 - ETA: 1:02 - loss: 2.7455 - acc: 0.372 - ETA: 1:02 - loss: 2.7463 - acc: 0.374 - ETA: 1:01 - loss: 2.7518 - acc: 0.372 - ETA: 1:00 - loss: 2.7490 - acc: 0.373 - ETA: 59s - loss: 2.7450 - acc: 0.371 - ETA: 58s - loss: 2.7379 - acc: 0.37 - ETA: 57s - loss: 2.7389 - acc: 0.37 - ETA: 56s - loss: 2.7435 - acc: 0.36 - ETA: 57s - loss: 2.7463 - acc: 0.36 - ETA: 57s - loss: 2.7492 - acc: 0.36 - ETA: 56s - loss: 2.7522 - acc: 0.36 - ETA: 55s - loss: 2.7532 - acc: 0.36 - ETA: 54s - loss: 2.7612 - acc: 0.36 - ETA: 53s - loss: 2.7655 - acc: 0.36 - ETA: 52s - loss: 2.7631 - acc: 0.36 - ETA: 51s - loss: 2.7662 - acc: 0.36 - ETA: 52s - loss: 2.7704 - acc: 0.36 - ETA: 51s - loss: 2.7673 - acc: 0.36 - ETA: 50s - loss: 2.7712 - acc: 0.37 - ETA: 49s - loss: 2.7788 - acc: 0.36 - ETA: 49s - loss: 2.7833 - acc: 0.36 - ETA: 48s - loss: 2.7846 - acc: 0.36 - ETA: 47s - loss: 2.7824 - acc: 0.36 - ETA: 47s - loss: 2.7808 - acc: 0.36 - ETA: 46s - loss: 2.7827 - acc: 0.36 - ETA: 46s - loss: 2.7790 - acc: 0.36 - ETA: 45s - loss: 2.7804 - acc: 0.36 - ETA: 44s - loss: 2.7857 - acc: 0.36 - ETA: 44s - loss: 2.7824 - acc: 0.36 - ETA: 43s - loss: 2.7800 - acc: 0.36 - ETA: 42s - loss: 2.7779 - acc: 0.36 - ETA: 42s - loss: 2.7793 - acc: 0.36 - ETA: 41s - loss: 2.7834 - acc: 0.36 - ETA: 40s - loss: 2.7834 - acc: 0.36 - ETA: 40s - loss: 2.7933 - acc: 0.36 - ETA: 39s - loss: 2.7928 - acc: 0.36 - ETA: 39s - loss: 2.7937 - acc: 0.36 - ETA: 38s - loss: 2.7934 - acc: 0.36 - ETA: 37s - loss: 2.7877 - acc: 0.37 - ETA: 36s - loss: 2.7909 - acc: 0.36 - ETA: 36s - loss: 2.7887 - acc: 0.36 - ETA: 35s - loss: 2.7861 - acc: 0.36 - ETA: 34s - loss: 2.7894 - acc: 0.36 - ETA: 34s - loss: 2.7897 - acc: 0.36 - ETA: 34s - loss: 2.7851 - acc: 0.36 - ETA: 33s - loss: 2.7831 - acc: 0.36 - ETA: 32s - loss: 2.7816 - acc: 0.36 - ETA: 31s - loss: 2.7834 - acc: 0.36 - ETA: 30s - loss: 2.7890 - acc: 0.36 - ETA: 30s - loss: 2.7905 - acc: 0.36 - ETA: 29s - loss: 2.7903 - acc: 0.36 - ETA: 28s - loss: 2.7906 - acc: 0.36 - ETA: 28s - loss: 2.7892 - acc: 0.36 - ETA: 27s - loss: 2.7893 - acc: 0.36 - ETA: 27s - loss: 2.7938 - acc: 0.36 - ETA: 26s - loss: 2.7937 - acc: 0.36 - ETA: 25s - loss: 2.7959 - acc: 0.36 - ETA: 24s - loss: 2.7939 - acc: 0.36 - ETA: 24s - loss: 2.7969 - acc: 0.36 - ETA: 23s - loss: 2.7992 - acc: 0.36 - ETA: 23s - loss: 2.7999 - acc: 0.36 - ETA: 22s - loss: 2.8010 - acc: 0.36 - ETA: 21s - loss: 2.7987 - acc: 0.36 - ETA: 21s - loss: 2.7953 - acc: 0.36 - ETA: 20s - loss: 2.7945 - acc: 0.36 - ETA: 19s - loss: 2.7931 - acc: 0.36 - ETA: 18s - loss: 2.7947 - acc: 0.36 - ETA: 18s - loss: 2.7958 - acc: 0.36 - ETA: 17s - loss: 2.7940 - acc: 0.36 - ETA: 17s - loss: 2.7937 - acc: 0.36 - ETA: 16s - loss: 2.7972 - acc: 0.36 - ETA: 15s - loss: 2.7959 - acc: 0.36 - ETA: 14s - loss: 2.7965 - acc: 0.36 - ETA: 14s - loss: 2.7959 - acc: 0.36 - ETA: 13s - loss: 2.7973 - acc: 0.36 - ETA: 12s - loss: 2.7960 - acc: 0.36 - ETA: 12s - loss: 2.7925 - acc: 0.36 - ETA: 11s - loss: 2.7941 - acc: 0.36 - ETA: 10s - loss: 2.7942 - acc: 0.36 - ETA: 10s - loss: 2.7927 - acc: 0.36 - ETA: 9s - loss: 2.7982 - acc: 0.3684 - ETA: 8s - loss: 2.7984 - acc: 0.367 - ETA: 8s - loss: 2.7958 - acc: 0.367 - ETA: 7s - loss: 2.7917 - acc: 0.368 - ETA: 6s - loss: 2.7935 - acc: 0.367 - ETA: 6s - loss: 2.7943 - acc: 0.368 - ETA: 5s - loss: 2.7909 - acc: 0.368 - ETA: 4s - loss: 2.7870 - acc: 0.369 - ETA: 4s - loss: 2.7852 - acc: 0.369 - ETA: 3s - loss: 2.7836 - acc: 0.369 - ETA: 2s - loss: 2.7836 - acc: 0.369 - ETA: 2s - loss: 2.7857 - acc: 0.368 - ETA: 1s - loss: 2.7844 - acc: 0.367 - ETA: 0s - loss: 2.7841 - acc: 0.367 - 117s 847ms/step - loss: 2.7840 - acc: 0.3673 - val_loss: 2.9230 - val_acc: 0.5276\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, train: 0.629\n",
      "\n",
      "PR AUC Flower, train: 0.676\n",
      "\n",
      "PR AUC Sugar, train: 0.726\n",
      "\n",
      "PR AUC Gravel, train: 0.531\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, train: 0.640\n",
      "####################\n",
      "\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, val: 0.638\n",
      "\n",
      "PR AUC Flower, val: 0.675\n",
      "\n",
      "PR AUC Sugar, val: 0.738\n",
      "\n",
      "PR AUC Gravel, val: 0.560\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, val: 0.653\n",
      "####################\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 42s - loss: 2.7293 - acc: 0.25 - ETA: 42s - loss: 2.8802 - acc: 0.28 - ETA: 42s - loss: 2.7325 - acc: 0.33 - ETA: 43s - loss: 2.7700 - acc: 0.35 - ETA: 43s - loss: 2.8303 - acc: 0.36 - ETA: 43s - loss: 2.8127 - acc: 0.35 - ETA: 44s - loss: 2.8310 - acc: 0.33 - ETA: 44s - loss: 2.9030 - acc: 0.34 - ETA: 44s - loss: 2.8662 - acc: 0.34 - ETA: 43s - loss: 2.8309 - acc: 0.35 - ETA: 43s - loss: 2.8868 - acc: 0.36 - ETA: 1:39 - loss: 2.8620 - acc: 0.364 - ETA: 1:38 - loss: 2.8731 - acc: 0.358 - ETA: 1:33 - loss: 2.8350 - acc: 0.361 - ETA: 1:29 - loss: 2.8269 - acc: 0.358 - ETA: 1:25 - loss: 2.8210 - acc: 0.357 - ETA: 1:22 - loss: 2.8173 - acc: 0.360 - ETA: 1:18 - loss: 2.8060 - acc: 0.362 - ETA: 1:16 - loss: 2.7907 - acc: 0.370 - ETA: 1:28 - loss: 2.7961 - acc: 0.364 - ETA: 1:29 - loss: 2.7799 - acc: 0.363 - ETA: 1:25 - loss: 2.7919 - acc: 0.355 - ETA: 1:22 - loss: 2.7991 - acc: 0.360 - ETA: 1:20 - loss: 2.8035 - acc: 0.355 - ETA: 1:17 - loss: 2.8068 - acc: 0.352 - ETA: 1:15 - loss: 2.8090 - acc: 0.349 - ETA: 1:13 - loss: 2.8078 - acc: 0.350 - ETA: 1:19 - loss: 2.8147 - acc: 0.352 - ETA: 1:19 - loss: 2.8154 - acc: 0.350 - ETA: 1:19 - loss: 2.8223 - acc: 0.347 - ETA: 1:17 - loss: 2.8100 - acc: 0.347 - ETA: 1:15 - loss: 2.8096 - acc: 0.349 - ETA: 1:13 - loss: 2.8185 - acc: 0.350 - ETA: 1:11 - loss: 2.8107 - acc: 0.352 - ETA: 1:09 - loss: 2.8182 - acc: 0.356 - ETA: 1:12 - loss: 2.8115 - acc: 0.358 - ETA: 1:12 - loss: 2.8047 - acc: 0.363 - ETA: 1:10 - loss: 2.8034 - acc: 0.362 - ETA: 1:09 - loss: 2.8131 - acc: 0.363 - ETA: 1:08 - loss: 2.8160 - acc: 0.364 - ETA: 1:06 - loss: 2.8036 - acc: 0.366 - ETA: 1:05 - loss: 2.8047 - acc: 0.364 - ETA: 1:03 - loss: 2.8072 - acc: 0.362 - ETA: 1:06 - loss: 2.8034 - acc: 0.363 - ETA: 1:06 - loss: 2.7943 - acc: 0.364 - ETA: 1:04 - loss: 2.7848 - acc: 0.364 - ETA: 1:03 - loss: 2.7812 - acc: 0.365 - ETA: 1:02 - loss: 2.7851 - acc: 0.365 - ETA: 1:01 - loss: 2.7854 - acc: 0.366 - ETA: 59s - loss: 2.7895 - acc: 0.365 - ETA: 59s - loss: 2.7836 - acc: 0.36 - ETA: 1:00 - loss: 2.7939 - acc: 0.366 - ETA: 1:00 - loss: 2.7982 - acc: 0.367 - ETA: 59s - loss: 2.7974 - acc: 0.367 - ETA: 57s - loss: 2.8005 - acc: 0.36 - ETA: 57s - loss: 2.7956 - acc: 0.36 - ETA: 56s - loss: 2.7967 - acc: 0.36 - ETA: 55s - loss: 2.7951 - acc: 0.36 - ETA: 54s - loss: 2.7998 - acc: 0.36 - ETA: 54s - loss: 2.7975 - acc: 0.36 - ETA: 54s - loss: 2.7943 - acc: 0.36 - ETA: 52s - loss: 2.7924 - acc: 0.36 - ETA: 51s - loss: 2.7919 - acc: 0.36 - ETA: 51s - loss: 2.7922 - acc: 0.37 - ETA: 50s - loss: 2.7941 - acc: 0.36 - ETA: 49s - loss: 2.7954 - acc: 0.37 - ETA: 48s - loss: 2.7916 - acc: 0.37 - ETA: 48s - loss: 2.7882 - acc: 0.37 - ETA: 48s - loss: 2.7854 - acc: 0.37 - ETA: 47s - loss: 2.7844 - acc: 0.37 - ETA: 46s - loss: 2.7853 - acc: 0.37 - ETA: 46s - loss: 2.7842 - acc: 0.37 - ETA: 45s - loss: 2.7879 - acc: 0.37 - ETA: 44s - loss: 2.7900 - acc: 0.37 - ETA: 43s - loss: 2.7923 - acc: 0.37 - ETA: 42s - loss: 2.7933 - acc: 0.37 - ETA: 42s - loss: 2.7994 - acc: 0.37 - ETA: 41s - loss: 2.7996 - acc: 0.37 - ETA: 40s - loss: 2.8035 - acc: 0.36 - ETA: 40s - loss: 2.8010 - acc: 0.36 - ETA: 39s - loss: 2.8017 - acc: 0.36 - ETA: 38s - loss: 2.8006 - acc: 0.36 - ETA: 38s - loss: 2.7990 - acc: 0.36 - ETA: 37s - loss: 2.7962 - acc: 0.36 - ETA: 36s - loss: 2.7949 - acc: 0.36 - ETA: 35s - loss: 2.7909 - acc: 0.36 - ETA: 34s - loss: 2.7854 - acc: 0.36 - ETA: 35s - loss: 2.7863 - acc: 0.36 - ETA: 34s - loss: 2.7830 - acc: 0.36 - ETA: 33s - loss: 2.7786 - acc: 0.36 - ETA: 32s - loss: 2.7772 - acc: 0.36 - ETA: 31s - loss: 2.7744 - acc: 0.36 - ETA: 31s - loss: 2.7738 - acc: 0.36 - ETA: 30s - loss: 2.7731 - acc: 0.36 - ETA: 29s - loss: 2.7752 - acc: 0.36 - ETA: 29s - loss: 2.7761 - acc: 0.36 - ETA: 28s - loss: 2.7779 - acc: 0.36 - ETA: 27s - loss: 2.7729 - acc: 0.36 - ETA: 27s - loss: 2.7703 - acc: 0.36 - ETA: 26s - loss: 2.7724 - acc: 0.36 - ETA: 25s - loss: 2.7720 - acc: 0.36 - ETA: 24s - loss: 2.7774 - acc: 0.36 - ETA: 23s - loss: 2.7779 - acc: 0.36 - ETA: 23s - loss: 2.7791 - acc: 0.36 - ETA: 23s - loss: 2.7831 - acc: 0.36 - ETA: 22s - loss: 2.7837 - acc: 0.36 - ETA: 21s - loss: 2.7857 - acc: 0.36 - ETA: 20s - loss: 2.7840 - acc: 0.36 - ETA: 19s - loss: 2.7847 - acc: 0.36 - ETA: 19s - loss: 2.7827 - acc: 0.36 - ETA: 18s - loss: 2.7834 - acc: 0.36 - ETA: 18s - loss: 2.7867 - acc: 0.36 - ETA: 17s - loss: 2.7877 - acc: 0.36 - ETA: 16s - loss: 2.7884 - acc: 0.36 - ETA: 15s - loss: 2.7898 - acc: 0.36 - ETA: 15s - loss: 2.7876 - acc: 0.36 - ETA: 14s - loss: 2.7868 - acc: 0.36 - ETA: 13s - loss: 2.7882 - acc: 0.36 - ETA: 12s - loss: 2.7871 - acc: 0.36 - ETA: 12s - loss: 2.7874 - acc: 0.36 - ETA: 11s - loss: 2.7864 - acc: 0.36 - ETA: 11s - loss: 2.7861 - acc: 0.36 - ETA: 10s - loss: 2.7842 - acc: 0.36 - ETA: 9s - loss: 2.7829 - acc: 0.3627 - ETA: 8s - loss: 2.7827 - acc: 0.363 - ETA: 8s - loss: 2.7807 - acc: 0.364 - ETA: 7s - loss: 2.7808 - acc: 0.363 - ETA: 7s - loss: 2.7790 - acc: 0.362 - ETA: 6s - loss: 2.7758 - acc: 0.362 - ETA: 5s - loss: 2.7786 - acc: 0.361 - ETA: 4s - loss: 2.7802 - acc: 0.360 - ETA: 4s - loss: 2.7789 - acc: 0.360 - ETA: 3s - loss: 2.7813 - acc: 0.359 - ETA: 2s - loss: 2.7829 - acc: 0.359 - ETA: 2s - loss: 2.7815 - acc: 0.358 - ETA: 1s - loss: 2.7793 - acc: 0.359 - ETA: 0s - loss: 2.7810 - acc: 0.359 - 118s 855ms/step - loss: 2.7813 - acc: 0.3605 - val_loss: 2.9151 - val_acc: 0.5230\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, train: 0.620\n",
      "\n",
      "PR AUC Flower, train: 0.673\n",
      "\n",
      "PR AUC Sugar, train: 0.728\n",
      "\n",
      "PR AUC Gravel, train: 0.529\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, train: 0.637\n",
      "####################\n",
      "\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, val: 0.630\n",
      "\n",
      "PR AUC Flower, val: 0.674\n",
      "\n",
      "PR AUC Sugar, val: 0.736\n",
      "\n",
      "PR AUC Gravel, val: 0.552\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, val: 0.648\n",
      "####################\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 49s - loss: 2.7643 - acc: 0.34 - ETA: 46s - loss: 2.6063 - acc: 0.42 - ETA: 46s - loss: 2.6510 - acc: 0.38 - ETA: 46s - loss: 2.7843 - acc: 0.37 - ETA: 45s - loss: 2.8075 - acc: 0.35 - ETA: 46s - loss: 2.8444 - acc: 0.36 - ETA: 46s - loss: 2.8334 - acc: 0.38 - ETA: 46s - loss: 2.8426 - acc: 0.38 - ETA: 48s - loss: 2.8107 - acc: 0.39 - ETA: 48s - loss: 2.8177 - acc: 0.38 - ETA: 48s - loss: 2.8156 - acc: 0.38 - ETA: 1:38 - loss: 2.8273 - acc: 0.380 - ETA: 1:39 - loss: 2.8539 - acc: 0.379 - ETA: 1:34 - loss: 2.8153 - acc: 0.388 - ETA: 1:30 - loss: 2.8257 - acc: 0.393 - ETA: 1:26 - loss: 2.8068 - acc: 0.394 - ETA: 1:22 - loss: 2.7798 - acc: 0.402 - ETA: 1:19 - loss: 2.7832 - acc: 0.397 - ETA: 1:16 - loss: 2.7952 - acc: 0.396 - ETA: 1:26 - loss: 2.8046 - acc: 0.393 - ETA: 1:26 - loss: 2.7980 - acc: 0.397 - ETA: 1:25 - loss: 2.8016 - acc: 0.400 - ETA: 1:22 - loss: 2.7993 - acc: 0.399 - ETA: 1:20 - loss: 2.8062 - acc: 0.399 - ETA: 1:17 - loss: 2.7865 - acc: 0.402 - ETA: 1:15 - loss: 2.7876 - acc: 0.401 - ETA: 1:13 - loss: 2.7776 - acc: 0.401 - ETA: 1:18 - loss: 2.7710 - acc: 0.401 - ETA: 1:18 - loss: 2.7814 - acc: 0.401 - ETA: 1:17 - loss: 2.7755 - acc: 0.401 - ETA: 1:15 - loss: 2.7608 - acc: 0.401 - ETA: 1:13 - loss: 2.7541 - acc: 0.404 - ETA: 1:11 - loss: 2.7537 - acc: 0.404 - ETA: 1:10 - loss: 2.7546 - acc: 0.402 - ETA: 1:08 - loss: 2.7488 - acc: 0.400 - ETA: 1:12 - loss: 2.7609 - acc: 0.396 - ETA: 1:11 - loss: 2.7605 - acc: 0.400 - ETA: 1:11 - loss: 2.7655 - acc: 0.397 - ETA: 1:09 - loss: 2.7664 - acc: 0.398 - ETA: 1:07 - loss: 2.7624 - acc: 0.397 - ETA: 1:06 - loss: 2.7721 - acc: 0.394 - ETA: 1:05 - loss: 2.7789 - acc: 0.392 - ETA: 1:04 - loss: 2.7774 - acc: 0.390 - ETA: 1:06 - loss: 2.7773 - acc: 0.388 - ETA: 1:05 - loss: 2.7746 - acc: 0.388 - ETA: 1:05 - loss: 2.7728 - acc: 0.385 - ETA: 1:03 - loss: 2.7691 - acc: 0.383 - ETA: 1:02 - loss: 2.7737 - acc: 0.382 - ETA: 1:01 - loss: 2.7726 - acc: 0.382 - ETA: 1:00 - loss: 2.7751 - acc: 0.381 - ETA: 59s - loss: 2.7719 - acc: 0.384 - ETA: 1:00 - loss: 2.7763 - acc: 0.385 - ETA: 59s - loss: 2.7679 - acc: 0.388 - ETA: 59s - loss: 2.7681 - acc: 0.38 - ETA: 58s - loss: 2.7663 - acc: 0.38 - ETA: 57s - loss: 2.7788 - acc: 0.38 - ETA: 56s - loss: 2.7806 - acc: 0.38 - ETA: 55s - loss: 2.7767 - acc: 0.38 - ETA: 54s - loss: 2.7700 - acc: 0.38 - ETA: 54s - loss: 2.7712 - acc: 0.38 - ETA: 54s - loss: 2.7760 - acc: 0.38 - ETA: 53s - loss: 2.7802 - acc: 0.38 - ETA: 52s - loss: 2.7859 - acc: 0.38 - ETA: 51s - loss: 2.7866 - acc: 0.38 - ETA: 50s - loss: 2.7943 - acc: 0.37 - ETA: 49s - loss: 2.7917 - acc: 0.38 - ETA: 48s - loss: 2.7873 - acc: 0.38 - ETA: 48s - loss: 2.7832 - acc: 0.38 - ETA: 48s - loss: 2.7890 - acc: 0.38 - ETA: 47s - loss: 2.7889 - acc: 0.38 - ETA: 46s - loss: 2.7844 - acc: 0.37 - ETA: 46s - loss: 2.7822 - acc: 0.37 - ETA: 45s - loss: 2.7864 - acc: 0.37 - ETA: 44s - loss: 2.7851 - acc: 0.37 - ETA: 43s - loss: 2.7876 - acc: 0.37 - ETA: 42s - loss: 2.7894 - acc: 0.37 - ETA: 42s - loss: 2.7863 - acc: 0.37 - ETA: 41s - loss: 2.7850 - acc: 0.37 - ETA: 40s - loss: 2.7866 - acc: 0.37 - ETA: 40s - loss: 2.7824 - acc: 0.37 - ETA: 39s - loss: 2.7799 - acc: 0.37 - ETA: 38s - loss: 2.7830 - acc: 0.37 - ETA: 38s - loss: 2.7828 - acc: 0.37 - ETA: 37s - loss: 2.7886 - acc: 0.37 - ETA: 36s - loss: 2.7907 - acc: 0.37 - ETA: 36s - loss: 2.7888 - acc: 0.37 - ETA: 35s - loss: 2.7906 - acc: 0.37 - ETA: 35s - loss: 2.7904 - acc: 0.37 - ETA: 34s - loss: 2.7887 - acc: 0.37 - ETA: 33s - loss: 2.7905 - acc: 0.37 - ETA: 32s - loss: 2.7904 - acc: 0.37 - ETA: 31s - loss: 2.7907 - acc: 0.37 - ETA: 31s - loss: 2.7931 - acc: 0.37 - ETA: 30s - loss: 2.7942 - acc: 0.37 - ETA: 29s - loss: 2.7914 - acc: 0.37 - ETA: 29s - loss: 2.7905 - acc: 0.37 - ETA: 28s - loss: 2.7910 - acc: 0.37 - ETA: 28s - loss: 2.7903 - acc: 0.37 - ETA: 27s - loss: 2.7926 - acc: 0.37 - ETA: 26s - loss: 2.7903 - acc: 0.37 - ETA: 25s - loss: 2.7879 - acc: 0.37 - ETA: 24s - loss: 2.7863 - acc: 0.37 - ETA: 24s - loss: 2.7825 - acc: 0.37 - ETA: 24s - loss: 2.7830 - acc: 0.37 - ETA: 23s - loss: 2.7839 - acc: 0.37 - ETA: 22s - loss: 2.7821 - acc: 0.37 - ETA: 21s - loss: 2.7786 - acc: 0.37 - ETA: 20s - loss: 2.7792 - acc: 0.37 - ETA: 20s - loss: 2.7782 - acc: 0.37 - ETA: 19s - loss: 2.7768 - acc: 0.37 - ETA: 18s - loss: 2.7784 - acc: 0.37 - ETA: 18s - loss: 2.7792 - acc: 0.37 - ETA: 17s - loss: 2.7809 - acc: 0.37 - ETA: 16s - loss: 2.7819 - acc: 0.37 - ETA: 16s - loss: 2.7828 - acc: 0.37 - ETA: 15s - loss: 2.7888 - acc: 0.37 - ETA: 14s - loss: 2.7871 - acc: 0.37 - ETA: 13s - loss: 2.7887 - acc: 0.37 - ETA: 13s - loss: 2.7844 - acc: 0.37 - ETA: 12s - loss: 2.7864 - acc: 0.37 - ETA: 11s - loss: 2.7856 - acc: 0.37 - ETA: 11s - loss: 2.7822 - acc: 0.37 - ETA: 10s - loss: 2.7800 - acc: 0.37 - ETA: 9s - loss: 2.7805 - acc: 0.3795 - ETA: 9s - loss: 2.7791 - acc: 0.379 - ETA: 8s - loss: 2.7781 - acc: 0.378 - ETA: 7s - loss: 2.7793 - acc: 0.378 - ETA: 7s - loss: 2.7793 - acc: 0.378 - ETA: 6s - loss: 2.7799 - acc: 0.376 - ETA: 5s - loss: 2.7814 - acc: 0.376 - ETA: 4s - loss: 2.7828 - acc: 0.376 - ETA: 4s - loss: 2.7815 - acc: 0.376 - ETA: 3s - loss: 2.7819 - acc: 0.376 - ETA: 2s - loss: 2.7840 - acc: 0.375 - ETA: 2s - loss: 2.7821 - acc: 0.375 - ETA: 1s - loss: 2.7789 - acc: 0.376 - ETA: 0s - loss: 2.7796 - acc: 0.376 - 119s 861ms/step - loss: 2.7801 - acc: 0.3766 - val_loss: 2.9200 - val_acc: 0.5184\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, train: 0.626\n",
      "\n",
      "PR AUC Flower, train: 0.680\n",
      "\n",
      "PR AUC Sugar, train: 0.734\n",
      "\n",
      "PR AUC Gravel, train: 0.545\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, train: 0.646\n",
      "####################\n",
      "\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, val: 0.635\n",
      "\n",
      "PR AUC Flower, val: 0.677\n",
      "\n",
      "PR AUC Sugar, val: 0.739\n",
      "\n",
      "PR AUC Gravel, val: 0.563\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, val: 0.653\n",
      "####################\n",
      "\n",
      "\n",
      "####################\n",
      "Reduced learning rate to 0.0005000000237487257.\n",
      "####################\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 42s - loss: 2.9585 - acc: 0.18 - ETA: 42s - loss: 2.8136 - acc: 0.25 - ETA: 42s - loss: 2.7358 - acc: 0.31 - ETA: 43s - loss: 2.8183 - acc: 0.32 - ETA: 43s - loss: 2.8436 - acc: 0.31 - ETA: 44s - loss: 2.7900 - acc: 0.35 - ETA: 46s - loss: 2.7750 - acc: 0.35 - ETA: 45s - loss: 2.7679 - acc: 0.34 - ETA: 45s - loss: 2.7456 - acc: 0.37 - ETA: 44s - loss: 2.7402 - acc: 0.37 - ETA: 44s - loss: 2.7407 - acc: 0.37 - ETA: 1:26 - loss: 2.7491 - acc: 0.372 - ETA: 1:29 - loss: 2.7376 - acc: 0.370 - ETA: 1:26 - loss: 2.7516 - acc: 0.357 - ETA: 1:22 - loss: 2.7262 - acc: 0.366 - ETA: 1:19 - loss: 2.7300 - acc: 0.380 - ETA: 1:16 - loss: 2.7398 - acc: 0.384 - ETA: 1:13 - loss: 2.7592 - acc: 0.383 - ETA: 1:11 - loss: 2.7781 - acc: 0.379 - ETA: 1:19 - loss: 2.7856 - acc: 0.382 - ETA: 1:19 - loss: 2.7644 - acc: 0.389 - ETA: 1:19 - loss: 2.7692 - acc: 0.386 - ETA: 1:16 - loss: 2.7694 - acc: 0.380 - ETA: 1:14 - loss: 2.7730 - acc: 0.377 - ETA: 1:12 - loss: 2.7667 - acc: 0.375 - ETA: 1:10 - loss: 2.7617 - acc: 0.376 - ETA: 1:08 - loss: 2.7430 - acc: 0.383 - ETA: 1:13 - loss: 2.7330 - acc: 0.379 - ETA: 1:14 - loss: 2.7263 - acc: 0.377 - ETA: 1:12 - loss: 2.7321 - acc: 0.380 - ETA: 1:11 - loss: 2.7295 - acc: 0.377 - ETA: 1:10 - loss: 2.7363 - acc: 0.376 - ETA: 1:09 - loss: 2.7301 - acc: 0.381 - ETA: 1:07 - loss: 2.7300 - acc: 0.382 - ETA: 1:05 - loss: 2.7380 - acc: 0.380 - ETA: 1:08 - loss: 2.7372 - acc: 0.379 - ETA: 1:08 - loss: 2.7334 - acc: 0.380 - ETA: 1:07 - loss: 2.7365 - acc: 0.379 - ETA: 1:07 - loss: 2.7322 - acc: 0.381 - ETA: 1:05 - loss: 2.7378 - acc: 0.380 - ETA: 1:04 - loss: 2.7293 - acc: 0.381 - ETA: 1:02 - loss: 2.7337 - acc: 0.378 - ETA: 1:01 - loss: 2.7438 - acc: 0.374 - ETA: 1:02 - loss: 2.7496 - acc: 0.375 - ETA: 1:03 - loss: 2.7468 - acc: 0.373 - ETA: 1:01 - loss: 2.7537 - acc: 0.376 - ETA: 1:01 - loss: 2.7587 - acc: 0.373 - ETA: 1:00 - loss: 2.7487 - acc: 0.375 - ETA: 59s - loss: 2.7526 - acc: 0.372 - ETA: 58s - loss: 2.7522 - acc: 0.37 - ETA: 57s - loss: 2.7567 - acc: 0.36 - ETA: 57s - loss: 2.7534 - acc: 0.36 - ETA: 57s - loss: 2.7613 - acc: 0.36 - ETA: 56s - loss: 2.7604 - acc: 0.36 - ETA: 56s - loss: 2.7576 - acc: 0.36 - ETA: 55s - loss: 2.7561 - acc: 0.36 - ETA: 55s - loss: 2.7496 - acc: 0.36 - ETA: 53s - loss: 2.7539 - acc: 0.36 - ETA: 52s - loss: 2.7619 - acc: 0.36 - ETA: 52s - loss: 2.7633 - acc: 0.36 - ETA: 52s - loss: 2.7718 - acc: 0.36 - ETA: 51s - loss: 2.7695 - acc: 0.36 - ETA: 50s - loss: 2.7668 - acc: 0.36 - ETA: 50s - loss: 2.7773 - acc: 0.36 - ETA: 49s - loss: 2.7792 - acc: 0.36 - ETA: 48s - loss: 2.7830 - acc: 0.36 - ETA: 47s - loss: 2.7816 - acc: 0.36 - ETA: 47s - loss: 2.7814 - acc: 0.36 - ETA: 46s - loss: 2.7769 - acc: 0.36 - ETA: 45s - loss: 2.7733 - acc: 0.36 - ETA: 45s - loss: 2.7770 - acc: 0.36 - ETA: 45s - loss: 2.7766 - acc: 0.36 - ETA: 44s - loss: 2.7763 - acc: 0.36 - ETA: 43s - loss: 2.7775 - acc: 0.36 - ETA: 42s - loss: 2.7744 - acc: 0.36 - ETA: 41s - loss: 2.7777 - acc: 0.36 - ETA: 41s - loss: 2.7747 - acc: 0.36 - ETA: 40s - loss: 2.7726 - acc: 0.36 - ETA: 39s - loss: 2.7696 - acc: 0.36 - ETA: 40s - loss: 2.7669 - acc: 0.36 - ETA: 39s - loss: 2.7672 - acc: 0.36 - ETA: 38s - loss: 2.7701 - acc: 0.37 - ETA: 37s - loss: 2.7646 - acc: 0.37 - ETA: 36s - loss: 2.7662 - acc: 0.37 - ETA: 35s - loss: 2.7654 - acc: 0.36 - ETA: 34s - loss: 2.7674 - acc: 0.36 - ETA: 34s - loss: 2.7683 - acc: 0.36 - ETA: 34s - loss: 2.7706 - acc: 0.36 - ETA: 33s - loss: 2.7709 - acc: 0.36 - ETA: 32s - loss: 2.7711 - acc: 0.36 - ETA: 32s - loss: 2.7711 - acc: 0.36 - ETA: 31s - loss: 2.7668 - acc: 0.36 - ETA: 30s - loss: 2.7743 - acc: 0.36 - ETA: 29s - loss: 2.7758 - acc: 0.36 - ETA: 28s - loss: 2.7735 - acc: 0.36 - ETA: 29s - loss: 2.7750 - acc: 0.36 - ETA: 28s - loss: 2.7764 - acc: 0.36 - ETA: 27s - loss: 2.7754 - acc: 0.36 - ETA: 26s - loss: 2.7742 - acc: 0.37 - ETA: 25s - loss: 2.7753 - acc: 0.37 - ETA: 25s - loss: 2.7747 - acc: 0.37 - ETA: 24s - loss: 2.7759 - acc: 0.37 - ETA: 23s - loss: 2.7773 - acc: 0.37 - ETA: 23s - loss: 2.7755 - acc: 0.37 - ETA: 22s - loss: 2.7763 - acc: 0.37 - ETA: 22s - loss: 2.7714 - acc: 0.37 - ETA: 21s - loss: 2.7712 - acc: 0.37 - ETA: 20s - loss: 2.7715 - acc: 0.37 - ETA: 19s - loss: 2.7716 - acc: 0.37 - ETA: 18s - loss: 2.7719 - acc: 0.37 - ETA: 18s - loss: 2.7705 - acc: 0.37 - ETA: 18s - loss: 2.7674 - acc: 0.37 - ETA: 17s - loss: 2.7673 - acc: 0.37 - ETA: 16s - loss: 2.7696 - acc: 0.37 - ETA: 15s - loss: 2.7730 - acc: 0.37 - ETA: 15s - loss: 2.7700 - acc: 0.37 - ETA: 14s - loss: 2.7716 - acc: 0.37 - ETA: 13s - loss: 2.7715 - acc: 0.37 - ETA: 12s - loss: 2.7691 - acc: 0.37 - ETA: 12s - loss: 2.7679 - acc: 0.37 - ETA: 11s - loss: 2.7702 - acc: 0.37 - ETA: 11s - loss: 2.7724 - acc: 0.37 - ETA: 10s - loss: 2.7732 - acc: 0.37 - ETA: 9s - loss: 2.7701 - acc: 0.3735 - ETA: 8s - loss: 2.7708 - acc: 0.374 - ETA: 8s - loss: 2.7713 - acc: 0.375 - ETA: 7s - loss: 2.7713 - acc: 0.375 - ETA: 6s - loss: 2.7678 - acc: 0.376 - ETA: 6s - loss: 2.7685 - acc: 0.376 - ETA: 5s - loss: 2.7710 - acc: 0.376 - ETA: 4s - loss: 2.7744 - acc: 0.375 - ETA: 4s - loss: 2.7748 - acc: 0.375 - ETA: 3s - loss: 2.7737 - acc: 0.375 - ETA: 2s - loss: 2.7769 - acc: 0.375 - ETA: 2s - loss: 2.7810 - acc: 0.374 - ETA: 1s - loss: 2.7798 - acc: 0.375 - ETA: 0s - loss: 2.7791 - acc: 0.376 - 117s 849ms/step - loss: 2.7770 - acc: 0.3761 - val_loss: 2.9396 - val_acc: 0.5285\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, train: 0.633\n",
      "\n",
      "PR AUC Flower, train: 0.679\n",
      "\n",
      "PR AUC Sugar, train: 0.738\n",
      "\n",
      "PR AUC Gravel, train: 0.555\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, train: 0.651\n",
      "####################\n",
      "\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, val: 0.640\n",
      "\n",
      "PR AUC Flower, val: 0.675\n",
      "\n",
      "PR AUC Sugar, val: 0.740\n",
      "\n",
      "PR AUC Gravel, val: 0.577\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, val: 0.658\n",
      "####################\n",
      "\n",
      "\n",
      "####################\n",
      "Saved new checkpoint\n",
      "####################\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - ETA: 40s - loss: 2.7194 - acc: 0.40 - ETA: 41s - loss: 2.7165 - acc: 0.37 - ETA: 42s - loss: 2.7294 - acc: 0.39 - ETA: 42s - loss: 2.7886 - acc: 0.37 - ETA: 42s - loss: 2.7252 - acc: 0.38 - ETA: 42s - loss: 2.7393 - acc: 0.38 - ETA: 42s - loss: 2.7192 - acc: 0.37 - ETA: 44s - loss: 2.7598 - acc: 0.35 - ETA: 44s - loss: 2.7402 - acc: 0.36 - ETA: 44s - loss: 2.7689 - acc: 0.35 - ETA: 44s - loss: 2.7856 - acc: 0.36 - ETA: 1:22 - loss: 2.8429 - acc: 0.364 - ETA: 1:24 - loss: 2.8410 - acc: 0.363 - ETA: 1:20 - loss: 2.8212 - acc: 0.366 - ETA: 1:16 - loss: 2.8273 - acc: 0.368 - ETA: 1:13 - loss: 2.8400 - acc: 0.373 - ETA: 1:11 - loss: 2.8430 - acc: 0.365 - ETA: 1:08 - loss: 2.8289 - acc: 0.364 - ETA: 1:06 - loss: 2.8224 - acc: 0.363 - ETA: 1:17 - loss: 2.8436 - acc: 0.365 - ETA: 1:17 - loss: 2.8267 - acc: 0.367 - ETA: 1:16 - loss: 2.8302 - acc: 0.369 - ETA: 1:14 - loss: 2.8287 - acc: 0.365 - ETA: 1:11 - loss: 2.8127 - acc: 0.363 - ETA: 1:09 - loss: 2.8288 - acc: 0.358 - ETA: 1:07 - loss: 2.8074 - acc: 0.366 - ETA: 1:05 - loss: 2.8031 - acc: 0.365 - ETA: 1:12 - loss: 2.8035 - acc: 0.369 - ETA: 1:12 - loss: 2.8031 - acc: 0.369 - ETA: 1:10 - loss: 2.7954 - acc: 0.375 - ETA: 1:10 - loss: 2.7918 - acc: 0.375 - ETA: 1:08 - loss: 2.7909 - acc: 0.377 - ETA: 1:06 - loss: 2.7949 - acc: 0.376 - ETA: 1:04 - loss: 2.8067 - acc: 0.373 - ETA: 1:03 - loss: 2.8103 - acc: 0.371 - ETA: 1:07 - loss: 2.8001 - acc: 0.371 - ETA: 1:07 - loss: 2.7953 - acc: 0.371 - ETA: 1:05 - loss: 2.7824 - acc: 0.378 - ETA: 1:04 - loss: 2.7837 - acc: 0.377 - ETA: 1:04 - loss: 2.7820 - acc: 0.374 - ETA: 1:02 - loss: 2.7849 - acc: 0.372 - ETA: 1:01 - loss: 2.7862 - acc: 0.373 - ETA: 1:00 - loss: 2.7893 - acc: 0.372 - ETA: 1:02 - loss: 2.7920 - acc: 0.374 - ETA: 1:02 - loss: 2.7798 - acc: 0.375 - ETA: 1:00 - loss: 2.7835 - acc: 0.375 - ETA: 59s - loss: 2.7887 - acc: 0.377 - ETA: 59s - loss: 2.7836 - acc: 0.37 - ETA: 58s - loss: 2.7860 - acc: 0.38 - ETA: 57s - loss: 2.7895 - acc: 0.38 - ETA: 56s - loss: 2.7966 - acc: 0.37 - ETA: 57s - loss: 2.7964 - acc: 0.37 - ETA: 56s - loss: 2.7915 - acc: 0.37 - ETA: 55s - loss: 2.7859 - acc: 0.38 - ETA: 54s - loss: 2.7795 - acc: 0.38 - ETA: 54s - loss: 2.7833 - acc: 0.38 - ETA: 53s - loss: 2.7812 - acc: 0.38 - ETA: 52s - loss: 2.7841 - acc: 0.38 - ETA: 51s - loss: 2.7843 - acc: 0.38 - ETA: 52s - loss: 2.7867 - acc: 0.38 - ETA: 51s - loss: 2.7852 - acc: 0.38 - ETA: 50s - loss: 2.7866 - acc: 0.37 - ETA: 49s - loss: 2.7848 - acc: 0.37 - ETA: 49s - loss: 2.7843 - acc: 0.37 - ETA: 48s - loss: 2.7822 - acc: 0.37 - ETA: 47s - loss: 2.7814 - acc: 0.37 - ETA: 46s - loss: 2.7745 - acc: 0.37 - ETA: 46s - loss: 2.7705 - acc: 0.37 - ETA: 46s - loss: 2.7711 - acc: 0.37 - ETA: 45s - loss: 2.7682 - acc: 0.37 - ETA: 44s - loss: 2.7658 - acc: 0.37 - ETA: 44s - loss: 2.7645 - acc: 0.37 - ETA: 43s - loss: 2.7710 - acc: 0.37 - ETA: 42s - loss: 2.7739 - acc: 0.37 - ETA: 42s - loss: 2.7713 - acc: 0.38 - ETA: 41s - loss: 2.7736 - acc: 0.37 - ETA: 40s - loss: 2.7726 - acc: 0.37 - ETA: 39s - loss: 2.7705 - acc: 0.37 - ETA: 39s - loss: 2.7692 - acc: 0.37 - ETA: 39s - loss: 2.7663 - acc: 0.38 - ETA: 38s - loss: 2.7645 - acc: 0.38 - ETA: 37s - loss: 2.7650 - acc: 0.38 - ETA: 36s - loss: 2.7636 - acc: 0.38 - ETA: 36s - loss: 2.7658 - acc: 0.38 - ETA: 35s - loss: 2.7663 - acc: 0.38 - ETA: 34s - loss: 2.7710 - acc: 0.37 - ETA: 33s - loss: 2.7715 - acc: 0.37 - ETA: 34s - loss: 2.7672 - acc: 0.37 - ETA: 33s - loss: 2.7630 - acc: 0.37 - ETA: 32s - loss: 2.7622 - acc: 0.37 - ETA: 31s - loss: 2.7567 - acc: 0.37 - ETA: 30s - loss: 2.7606 - acc: 0.37 - ETA: 30s - loss: 2.7586 - acc: 0.37 - ETA: 29s - loss: 2.7561 - acc: 0.37 - ETA: 28s - loss: 2.7541 - acc: 0.37 - ETA: 28s - loss: 2.7584 - acc: 0.37 - ETA: 28s - loss: 2.7623 - acc: 0.37 - ETA: 27s - loss: 2.7637 - acc: 0.37 - ETA: 26s - loss: 2.7597 - acc: 0.37 - ETA: 25s - loss: 2.7665 - acc: 0.37 - ETA: 24s - loss: 2.7679 - acc: 0.37 - ETA: 24s - loss: 2.7708 - acc: 0.37 - ETA: 23s - loss: 2.7689 - acc: 0.37 - ETA: 23s - loss: 2.7659 - acc: 0.37 - ETA: 22s - loss: 2.7682 - acc: 0.37 - ETA: 21s - loss: 2.7646 - acc: 0.37 - ETA: 21s - loss: 2.7648 - acc: 0.38 - ETA: 20s - loss: 2.7678 - acc: 0.38 - ETA: 19s - loss: 2.7640 - acc: 0.38 - ETA: 18s - loss: 2.7651 - acc: 0.38 - ETA: 18s - loss: 2.7657 - acc: 0.38 - ETA: 17s - loss: 2.7680 - acc: 0.38 - ETA: 17s - loss: 2.7674 - acc: 0.38 - ETA: 16s - loss: 2.7658 - acc: 0.38 - ETA: 15s - loss: 2.7651 - acc: 0.38 - ETA: 14s - loss: 2.7656 - acc: 0.38 - ETA: 14s - loss: 2.7662 - acc: 0.37 - ETA: 13s - loss: 2.7643 - acc: 0.38 - ETA: 12s - loss: 2.7637 - acc: 0.37 - ETA: 12s - loss: 2.7643 - acc: 0.37 - ETA: 11s - loss: 2.7657 - acc: 0.37 - ETA: 10s - loss: 2.7642 - acc: 0.37 - ETA: 10s - loss: 2.7645 - acc: 0.37 - ETA: 9s - loss: 2.7647 - acc: 0.3803 - ETA: 8s - loss: 2.7667 - acc: 0.379 - ETA: 8s - loss: 2.7654 - acc: 0.380 - ETA: 7s - loss: 2.7660 - acc: 0.379 - ETA: 6s - loss: 2.7643 - acc: 0.379 - ETA: 6s - loss: 2.7635 - acc: 0.380 - ETA: 5s - loss: 2.7663 - acc: 0.379 - ETA: 4s - loss: 2.7646 - acc: 0.378 - ETA: 4s - loss: 2.7646 - acc: 0.379 - ETA: 3s - loss: 2.7634 - acc: 0.379 - ETA: 2s - loss: 2.7635 - acc: 0.379 - ETA: 2s - loss: 2.7649 - acc: 0.378 - ETA: 1s - loss: 2.7672 - acc: 0.378 - ETA: 0s - loss: 2.7680 - acc: 0.377 - 117s 846ms/step - loss: 2.7649 - acc: 0.3784 - val_loss: 2.9502 - val_acc: 0.5349\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, train: 0.631\n",
      "\n",
      "PR AUC Flower, train: 0.675\n",
      "\n",
      "PR AUC Sugar, train: 0.737\n",
      "\n",
      "PR AUC Gravel, train: 0.560\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, train: 0.651\n",
      "####################\n",
      "\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, val: 0.637\n",
      "\n",
      "PR AUC Flower, val: 0.673\n",
      "\n",
      "PR AUC Sugar, val: 0.740\n",
      "\n",
      "PR AUC Gravel, val: 0.585\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, val: 0.659\n",
      "####################\n",
      "\n",
      "\n",
      "####################\n",
      "Saved new checkpoint\n",
      "####################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for base_layer in model.layers[:-3]:\n",
    "    base_layer.trainable = False\n",
    "    \n",
    "model.compile(optimizer=RAdam(warmup_proportion=0.1, min_lr=1e-5),  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_0 = model.fit_generator(generator=data_generator_train,\n",
    "                              validation_data=data_generator_val,\n",
    "                              epochs=10,\n",
    "                              callbacks=[train_metric_callback, val_callback],\n",
    "                              workers=num_cores,\n",
    "                              verbose=1\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the whole model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After unfreezing all the layers(except last 3) I set a less aggressive initial learning rate and train until early stopping (or 100 epochs max)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/277 [=====================>........] - ETA: 5:30:40 - loss: 1.9811 - acc: 0.62 - ETA: 2:46:14 - loss: 1.9109 - acc: 0.53 - ETA: 1:51:23 - loss: 2.0578 - acc: 0.54 - ETA: 1:23:58 - loss: 2.1168 - acc: 0.54 - ETA: 1:07:31 - loss: 2.1411 - acc: 0.55 - ETA: 56:33 - loss: 2.0947 - acc: 0.5208 - ETA: 48:43 - loss: 2.1552 - acc: 0.49 - ETA: 42:50 - loss: 2.1568 - acc: 0.47 - ETA: 38:15 - loss: 2.1254 - acc: 0.48 - ETA: 34:35 - loss: 2.0804 - acc: 0.51 - ETA: 31:35 - loss: 2.1318 - acc: 0.51 - ETA: 29:05 - loss: 2.1302 - acc: 0.52 - ETA: 26:58 - loss: 2.1574 - acc: 0.53 - ETA: 25:09 - loss: 2.1882 - acc: 0.54 - ETA: 23:34 - loss: 2.2072 - acc: 0.53 - ETA: 22:12 - loss: 2.2090 - acc: 0.53 - ETA: 20:59 - loss: 2.1849 - acc: 0.53 - ETA: 19:53 - loss: 2.2011 - acc: 0.53 - ETA: 18:55 - loss: 2.2309 - acc: 0.53 - ETA: 18:02 - loss: 2.2430 - acc: 0.52 - ETA: 17:15 - loss: 2.2262 - acc: 0.52 - ETA: 16:32 - loss: 2.2267 - acc: 0.53 - ETA: 15:52 - loss: 2.2082 - acc: 0.52 - ETA: 15:16 - loss: 2.2090 - acc: 0.51 - ETA: 14:42 - loss: 2.2179 - acc: 0.51 - ETA: 14:11 - loss: 2.2150 - acc: 0.50 - ETA: 13:42 - loss: 2.2240 - acc: 0.50 - ETA: 13:16 - loss: 2.2257 - acc: 0.51 - ETA: 12:51 - loss: 2.2263 - acc: 0.51 - ETA: 12:28 - loss: 2.2433 - acc: 0.51 - ETA: 12:06 - loss: 2.2464 - acc: 0.50 - ETA: 11:45 - loss: 2.2501 - acc: 0.50 - ETA: 11:26 - loss: 2.2527 - acc: 0.50 - ETA: 11:08 - loss: 2.2595 - acc: 0.50 - ETA: 10:50 - loss: 2.2554 - acc: 0.50 - ETA: 10:34 - loss: 2.2672 - acc: 0.50 - ETA: 10:19 - loss: 2.2721 - acc: 0.50 - ETA: 10:04 - loss: 2.2676 - acc: 0.50 - ETA: 9:50 - loss: 2.2747 - acc: 0.5048 - ETA: 9:37 - loss: 2.2777 - acc: 0.500 - ETA: 9:24 - loss: 2.2803 - acc: 0.506 - ETA: 9:12 - loss: 2.2816 - acc: 0.501 - ETA: 9:00 - loss: 2.2894 - acc: 0.498 - ETA: 8:49 - loss: 2.2918 - acc: 0.495 - ETA: 8:38 - loss: 2.3081 - acc: 0.491 - ETA: 8:28 - loss: 2.3002 - acc: 0.493 - ETA: 8:19 - loss: 2.3189 - acc: 0.489 - ETA: 8:09 - loss: 2.3196 - acc: 0.487 - ETA: 8:00 - loss: 2.3228 - acc: 0.484 - ETA: 7:51 - loss: 2.3296 - acc: 0.480 - ETA: 7:43 - loss: 2.3151 - acc: 0.485 - ETA: 7:35 - loss: 2.3223 - acc: 0.486 - ETA: 7:27 - loss: 2.3199 - acc: 0.491 - ETA: 7:20 - loss: 2.3213 - acc: 0.490 - ETA: 7:12 - loss: 2.3124 - acc: 0.490 - ETA: 7:05 - loss: 2.3232 - acc: 0.486 - ETA: 6:58 - loss: 2.3162 - acc: 0.483 - ETA: 6:52 - loss: 2.3126 - acc: 0.487 - ETA: 6:45 - loss: 2.3190 - acc: 0.485 - ETA: 6:39 - loss: 2.3046 - acc: 0.488 - ETA: 6:33 - loss: 2.2995 - acc: 0.486 - ETA: 6:27 - loss: 2.2952 - acc: 0.486 - ETA: 6:22 - loss: 2.2892 - acc: 0.487 - ETA: 6:16 - loss: 2.2864 - acc: 0.491 - ETA: 6:11 - loss: 2.2797 - acc: 0.493 - ETA: 6:05 - loss: 2.2796 - acc: 0.496 - ETA: 6:00 - loss: 2.2650 - acc: 0.498 - ETA: 5:55 - loss: 2.2679 - acc: 0.496 - ETA: 5:51 - loss: 2.2695 - acc: 0.494 - ETA: 5:46 - loss: 2.2753 - acc: 0.493 - ETA: 5:41 - loss: 2.2733 - acc: 0.492 - ETA: 5:37 - loss: 2.2723 - acc: 0.493 - ETA: 5:32 - loss: 2.2767 - acc: 0.491 - ETA: 5:28 - loss: 2.2779 - acc: 0.491 - ETA: 5:24 - loss: 2.2661 - acc: 0.491 - ETA: 5:20 - loss: 2.2605 - acc: 0.493 - ETA: 5:16 - loss: 2.2592 - acc: 0.494 - ETA: 5:12 - loss: 2.2633 - acc: 0.494 - ETA: 5:08 - loss: 2.2582 - acc: 0.492 - ETA: 5:04 - loss: 2.2614 - acc: 0.492 - ETA: 5:01 - loss: 2.2600 - acc: 0.492 - ETA: 4:57 - loss: 2.2556 - acc: 0.493 - ETA: 4:53 - loss: 2.2553 - acc: 0.496 - ETA: 4:50 - loss: 2.2524 - acc: 0.494 - ETA: 4:47 - loss: 2.2502 - acc: 0.493 - ETA: 4:43 - loss: 2.2512 - acc: 0.493 - ETA: 4:40 - loss: 2.2456 - acc: 0.493 - ETA: 4:37 - loss: 2.2413 - acc: 0.496 - ETA: 4:34 - loss: 2.2382 - acc: 0.497 - ETA: 4:30 - loss: 2.2431 - acc: 0.496 - ETA: 4:27 - loss: 2.2399 - acc: 0.498 - ETA: 4:24 - loss: 2.2401 - acc: 0.498 - ETA: 4:21 - loss: 2.2425 - acc: 0.498 - ETA: 4:18 - loss: 2.2442 - acc: 0.496 - ETA: 4:16 - loss: 2.2405 - acc: 0.495 - ETA: 4:13 - loss: 2.2438 - acc: 0.494 - ETA: 4:10 - loss: 2.2460 - acc: 0.492 - ETA: 4:07 - loss: 2.2519 - acc: 0.493 - ETA: 4:05 - loss: 2.2536 - acc: 0.494 - ETA: 4:02 - loss: 2.2572 - acc: 0.493 - ETA: 3:59 - loss: 2.2583 - acc: 0.493 - ETA: 3:57 - loss: 2.2585 - acc: 0.493 - ETA: 3:54 - loss: 2.2615 - acc: 0.495 - ETA: 3:52 - loss: 2.2569 - acc: 0.497 - ETA: 3:49 - loss: 2.2494 - acc: 0.497 - ETA: 3:47 - loss: 2.2563 - acc: 0.494 - ETA: 3:44 - loss: 2.2580 - acc: 0.494 - ETA: 3:42 - loss: 2.2622 - acc: 0.491 - ETA: 3:40 - loss: 2.2601 - acc: 0.493 - ETA: 3:37 - loss: 2.2558 - acc: 0.495 - ETA: 3:35 - loss: 2.2567 - acc: 0.495 - ETA: 3:33 - loss: 2.2535 - acc: 0.496 - ETA: 3:31 - loss: 2.2512 - acc: 0.496 - ETA: 3:29 - loss: 2.2496 - acc: 0.497 - ETA: 3:26 - loss: 2.2487 - acc: 0.496 - ETA: 3:24 - loss: 2.2452 - acc: 0.497 - ETA: 3:22 - loss: 2.2421 - acc: 0.497 - ETA: 3:20 - loss: 2.2433 - acc: 0.498 - ETA: 3:18 - loss: 2.2459 - acc: 0.498 - ETA: 3:16 - loss: 2.2502 - acc: 0.498 - ETA: 3:14 - loss: 2.2531 - acc: 0.496 - ETA: 3:12 - loss: 2.2536 - acc: 0.496 - ETA: 3:10 - loss: 2.2510 - acc: 0.497 - ETA: 3:08 - loss: 2.2499 - acc: 0.499 - ETA: 3:06 - loss: 2.2474 - acc: 0.500 - ETA: 3:04 - loss: 2.2475 - acc: 0.499 - ETA: 3:02 - loss: 2.2477 - acc: 0.500 - ETA: 3:00 - loss: 2.2470 - acc: 0.500 - ETA: 2:59 - loss: 2.2476 - acc: 0.500 - ETA: 2:57 - loss: 2.2548 - acc: 0.500 - ETA: 2:55 - loss: 2.2508 - acc: 0.501 - ETA: 2:53 - loss: 2.2501 - acc: 0.501 - ETA: 2:51 - loss: 2.2491 - acc: 0.500 - ETA: 2:50 - loss: 2.2470 - acc: 0.501 - ETA: 2:48 - loss: 2.2498 - acc: 0.500 - ETA: 2:46 - loss: 2.2531 - acc: 0.498 - ETA: 2:45 - loss: 2.2534 - acc: 0.497 - ETA: 2:43 - loss: 2.2484 - acc: 0.499 - ETA: 2:41 - loss: 2.2519 - acc: 0.499 - ETA: 2:39 - loss: 2.2511 - acc: 0.498 - ETA: 2:38 - loss: 2.2571 - acc: 0.498 - ETA: 2:36 - loss: 2.2568 - acc: 0.498 - ETA: 2:35 - loss: 2.2589 - acc: 0.498 - ETA: 2:33 - loss: 2.2596 - acc: 0.499 - ETA: 2:31 - loss: 2.2560 - acc: 0.500 - ETA: 2:30 - loss: 2.2579 - acc: 0.500 - ETA: 2:28 - loss: 2.2601 - acc: 0.499 - ETA: 2:27 - loss: 2.2619 - acc: 0.499 - ETA: 2:25 - loss: 2.2597 - acc: 0.498 - ETA: 2:23 - loss: 2.2597 - acc: 0.497 - ETA: 2:22 - loss: 2.2583 - acc: 0.498 - ETA: 2:20 - loss: 2.2571 - acc: 0.499 - ETA: 2:19 - loss: 2.2542 - acc: 0.500 - ETA: 2:17 - loss: 2.2552 - acc: 0.500 - ETA: 2:16 - loss: 2.2534 - acc: 0.500 - ETA: 2:14 - loss: 2.2555 - acc: 0.499 - ETA: 2:13 - loss: 2.2573 - acc: 0.497 - ETA: 2:12 - loss: 2.2578 - acc: 0.497 - ETA: 2:10 - loss: 2.2554 - acc: 0.497 - ETA: 2:09 - loss: 2.2537 - acc: 0.498 - ETA: 2:07 - loss: 2.2514 - acc: 0.499 - ETA: 2:06 - loss: 2.2534 - acc: 0.499 - ETA: 2:04 - loss: 2.2532 - acc: 0.498 - ETA: 2:03 - loss: 2.2510 - acc: 0.498 - ETA: 2:02 - loss: 2.2523 - acc: 0.497 - ETA: 2:00 - loss: 2.2521 - acc: 0.496 - ETA: 1:59 - loss: 2.2543 - acc: 0.496 - ETA: 1:58 - loss: 2.2586 - acc: 0.495 - ETA: 1:56 - loss: 2.2610 - acc: 0.495 - ETA: 1:55 - loss: 2.2614 - acc: 0.494 - ETA: 1:54 - loss: 2.2607 - acc: 0.495 - ETA: 1:52 - loss: 2.2613 - acc: 0.496 - ETA: 1:51 - loss: 2.2606 - acc: 0.496 - ETA: 1:50 - loss: 2.2590 - acc: 0.496 - ETA: 1:48 - loss: 2.2600 - acc: 0.496 - ETA: 1:47 - loss: 2.2585 - acc: 0.496 - ETA: 1:46 - loss: 2.2625 - acc: 0.495 - ETA: 1:44 - loss: 2.2647 - acc: 0.495 - ETA: 1:43 - loss: 2.2618 - acc: 0.496 - ETA: 1:42 - loss: 2.2641 - acc: 0.494 - ETA: 1:41 - loss: 2.2645 - acc: 0.495 - ETA: 1:39 - loss: 2.2679 - acc: 0.493 - ETA: 1:38 - loss: 2.2692 - acc: 0.492 - ETA: 1:37 - loss: 2.2719 - acc: 0.493 - ETA: 1:36 - loss: 2.2719 - acc: 0.493 - ETA: 1:34 - loss: 2.2725 - acc: 0.493 - ETA: 1:33 - loss: 2.2751 - acc: 0.493 - ETA: 1:32 - loss: 2.2744 - acc: 0.494 - ETA: 1:31 - loss: 2.2761 - acc: 0.494 - ETA: 1:29 - loss: 2.2731 - acc: 0.494 - ETA: 1:28 - loss: 2.2743 - acc: 0.494 - ETA: 1:27 - loss: 2.2725 - acc: 0.495 - ETA: 1:26 - loss: 2.2756 - acc: 0.494 - ETA: 1:25 - loss: 2.2787 - acc: 0.493 - ETA: 1:24 - loss: 2.2773 - acc: 0.494 - ETA: 1:22 - loss: 2.2786 - acc: 0.493 - ETA: 1:21 - loss: 2.2821 - acc: 0.492 - ETA: 1:20 - loss: 2.2825 - acc: 0.492 - ETA: 1:19 - loss: 2.2854 - acc: 0.491 - ETA: 1:18 - loss: 2.2852 - acc: 0.492 - ETA: 1:17 - loss: 2.2861 - acc: 0.491 - ETA: 1:15 - loss: 2.2855 - acc: 0.492 - ETA: 1:14 - loss: 2.2858 - acc: 0.492 - ETA: 1:13 - loss: 2.2843 - acc: 0.4926\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/277 [==============================] - ETA: 1:12 - loss: 2.2820 - acc: 0.492 - ETA: 1:11 - loss: 2.2824 - acc: 0.492 - ETA: 1:10 - loss: 2.2848 - acc: 0.491 - ETA: 1:09 - loss: 2.2841 - acc: 0.492 - ETA: 1:08 - loss: 2.2835 - acc: 0.493 - ETA: 1:06 - loss: 2.2836 - acc: 0.493 - ETA: 1:05 - loss: 2.2839 - acc: 0.493 - ETA: 1:04 - loss: 2.2836 - acc: 0.492 - ETA: 1:03 - loss: 2.2836 - acc: 0.492 - ETA: 1:02 - loss: 2.2847 - acc: 0.492 - ETA: 1:01 - loss: 2.2843 - acc: 0.492 - ETA: 1:00 - loss: 2.2863 - acc: 0.492 - ETA: 59s - loss: 2.2868 - acc: 0.491 - ETA: 58s - loss: 2.2875 - acc: 0.49 - ETA: 57s - loss: 2.2897 - acc: 0.49 - ETA: 56s - loss: 2.2886 - acc: 0.49 - ETA: 54s - loss: 2.2875 - acc: 0.49 - ETA: 53s - loss: 2.2895 - acc: 0.49 - ETA: 52s - loss: 2.2894 - acc: 0.49 - ETA: 51s - loss: 2.2918 - acc: 0.49 - ETA: 50s - loss: 2.2929 - acc: 0.49 - ETA: 49s - loss: 2.2904 - acc: 0.49 - ETA: 48s - loss: 2.2886 - acc: 0.49 - ETA: 47s - loss: 2.2906 - acc: 0.49 - ETA: 46s - loss: 2.2899 - acc: 0.49 - ETA: 45s - loss: 2.2928 - acc: 0.49 - ETA: 44s - loss: 2.2919 - acc: 0.49 - ETA: 43s - loss: 2.2944 - acc: 0.49 - ETA: 42s - loss: 2.2964 - acc: 0.49 - ETA: 41s - loss: 2.2940 - acc: 0.49 - ETA: 40s - loss: 2.2956 - acc: 0.49 - ETA: 39s - loss: 2.2942 - acc: 0.49 - ETA: 38s - loss: 2.2947 - acc: 0.49 - ETA: 37s - loss: 2.2959 - acc: 0.49 - ETA: 36s - loss: 2.2979 - acc: 0.49 - ETA: 35s - loss: 2.2992 - acc: 0.49 - ETA: 34s - loss: 2.2975 - acc: 0.49 - ETA: 33s - loss: 2.2949 - acc: 0.49 - ETA: 32s - loss: 2.2942 - acc: 0.49 - ETA: 31s - loss: 2.2942 - acc: 0.49 - ETA: 30s - loss: 2.2964 - acc: 0.49 - ETA: 29s - loss: 2.2942 - acc: 0.49 - ETA: 28s - loss: 2.2949 - acc: 0.49 - ETA: 27s - loss: 2.2942 - acc: 0.49 - ETA: 26s - loss: 2.2956 - acc: 0.49 - ETA: 25s - loss: 2.2959 - acc: 0.49 - ETA: 24s - loss: 2.2932 - acc: 0.49 - ETA: 23s - loss: 2.2925 - acc: 0.49 - ETA: 22s - loss: 2.2926 - acc: 0.49 - ETA: 21s - loss: 2.2906 - acc: 0.49 - ETA: 20s - loss: 2.2909 - acc: 0.49 - ETA: 19s - loss: 2.2911 - acc: 0.49 - ETA: 18s - loss: 2.2913 - acc: 0.49 - ETA: 17s - loss: 2.2932 - acc: 0.49 - ETA: 16s - loss: 2.2971 - acc: 0.49 - ETA: 15s - loss: 2.2978 - acc: 0.49 - ETA: 14s - loss: 2.2978 - acc: 0.49 - ETA: 13s - loss: 2.2972 - acc: 0.49 - ETA: 13s - loss: 2.2974 - acc: 0.49 - ETA: 12s - loss: 2.2983 - acc: 0.49 - ETA: 11s - loss: 2.2982 - acc: 0.49 - ETA: 10s - loss: 2.3019 - acc: 0.49 - ETA: 9s - loss: 2.3011 - acc: 0.4937 - ETA: 8s - loss: 2.3010 - acc: 0.493 - ETA: 7s - loss: 2.3009 - acc: 0.494 - ETA: 6s - loss: 2.3044 - acc: 0.493 - ETA: 5s - loss: 2.3049 - acc: 0.494 - ETA: 4s - loss: 2.3021 - acc: 0.494 - ETA: 3s - loss: 2.3028 - acc: 0.494 - ETA: 2s - loss: 2.3021 - acc: 0.494 - ETA: 1s - loss: 2.3016 - acc: 0.495 - ETA: 0s - loss: 2.3019 - acc: 0.495 - 285s 1s/step - loss: 2.3027 - acc: 0.4953 - val_loss: 2.9377 - val_acc: 0.4755\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, train: 0.881\n",
      "\n",
      "PR AUC Flower, train: 0.930\n",
      "\n",
      "PR AUC Sugar, train: 0.926\n",
      "\n",
      "PR AUC Gravel, train: 0.905\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, train: 0.910\n",
      "####################\n",
      "\n",
      "\n",
      "##############################\n",
      "\n",
      "PR AUC Fish, val: 0.709\n",
      "\n",
      "PR AUC Flower, val: 0.857\n",
      "\n",
      "PR AUC Sugar, val: 0.875\n",
      "\n",
      "PR AUC Gravel, val: 0.771\n",
      "\n",
      "\n",
      "####################\n",
      " PR AUC mean, val: 0.803\n",
      "####################\n",
      "\n",
      "\n",
      "####################\n",
      "Reduced learning rate to 0.0005000000237487257.\n",
      "####################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for base_layer in model.layers[:-3]:\n",
    "    base_layer.trainable = True\n",
    "    \n",
    "model.compile(optimizer=RAdam(warmup_proportion=0.1, min_lr=1e-5),  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_1 = model.fit_generator(generator=data_generator_train,\n",
    "                              validation_data=data_generator_val,\n",
    "                              epochs=6,\n",
    "                              callbacks=[train_metric_callback, val_callback],\n",
    "                              workers=num_cores,\n",
    "                              verbose=1,\n",
    "                              initial_epoch=1\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing train and val PR AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_dots(ax, np_array):\n",
    "    ax.scatter(list(range(1, len(np_array) + 1)), np_array, s=50)\n",
    "    ax.plot(list(range(1, len(np_array) + 1)), np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pr_auc_history_train = train_metric_callback.get_pr_auc_history()\n",
    "pr_auc_history_val = val_callback.get_pr_auc_history()\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plot_with_dots(plt, pr_auc_history_train[-1])\n",
    "plot_with_dots(plt, pr_auc_history_val[-1])\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=15)\n",
    "plt.ylabel('Mean PR AUC', fontsize=15)\n",
    "plt.legend(['Train', 'Val'])\n",
    "plt.title('Training and Validation PR AUC', fontsize=20)\n",
    "plt.savefig('pr_auc_hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plot_with_dots(plt, history_0.history['loss']+history_1.history['loss'])\n",
    "plot_with_dots(plt, history_0.history['val_loss']+history_1.history['val_loss'])\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=15)\n",
    "plt.ylabel('Binary Crossentropy', fontsize=15)\n",
    "plt.legend(['Train', 'Val'])\n",
    "plt.title('Training and Validation Loss', fontsize=20)\n",
    "plt.savefig('loss_hist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I left the model to train longer on my local GPU. I then upload the best model and plots from the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('../input/clouds-classifier-files/classifier_densenet169_epoch_21_val_pr_auc_0.8365921057512743.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"../input/clouds-classifier-files/loss_hist_densenet169.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"../input/clouds-classifier-files/pr_auc_hist_densenet169.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting postprocessing thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Fish', 'Flower', 'Sugar', 'Gravel']\n",
    "def get_threshold_for_recall(y_true, y_pred, class_i, recall_threshold=0.94, precision_threshold=0.90, plot=False):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true[:, class_i], y_pred[:, class_i])\n",
    "    i = len(thresholds) - 1\n",
    "    best_recall_threshold = None\n",
    "    while best_recall_threshold is None:\n",
    "        next_threshold = thresholds[i]\n",
    "        next_recall = recall[i]\n",
    "        if next_recall >= recall_threshold:\n",
    "            best_recall_threshold = next_threshold\n",
    "        i -= 1\n",
    "        \n",
    "    # consice, even though unnecessary passing through all the values\n",
    "    best_precision_threshold = [thres for prec, thres in zip(precision, thresholds) if prec >= precision_threshold][0]\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.step(recall, precision, color='r', alpha=0.3, where='post')\n",
    "        plt.fill_between(recall, precision, alpha=0.3, color='r')\n",
    "        plt.axhline(y=precision[i + 1])\n",
    "        recall_for_prec_thres = [rec for rec, thres in zip(recall, thresholds) \n",
    "                                 if thres == best_precision_threshold][0]\n",
    "        plt.axvline(x=recall_for_prec_thres, color='g')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.legend(['PR curve', \n",
    "                    f'Precision {precision[i + 1]: .2f} corresponding to selected recall threshold',\n",
    "                    f'Recall {recall_for_prec_thres: .2f} corresponding to selected precision threshold'])\n",
    "        plt.title(f'Precision-Recall curve for Class {class_names[class_i]}')\n",
    "    return best_recall_threshold, best_precision_threshold\n",
    "\n",
    "y_pred = model.predict_generator(data_generator_val, workers=num_cores)\n",
    "y_true = data_generator_val.get_labels()\n",
    "recall_thresholds = dict()\n",
    "precision_thresholds = dict()\n",
    "for i, class_name in tqdm(enumerate(class_names)):\n",
    "    recall_thresholds[class_name], precision_thresholds[class_name] = get_threshold_for_recall(y_true, y_pred, i, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing segmentation submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting cloud classes for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator_test = DataGenenerator(folder_imgs=test_imgs_folder, shuffle=False)\n",
    "y_pred_test = model.predict_generator(data_generator_test, workers=num_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating set of images without masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_labels_empty = set()\n",
    "for i, (img, predictions) in enumerate(zip(os.listdir(test_imgs_folder), y_pred_test)):\n",
    "    for class_i, class_name in enumerate(class_names):\n",
    "        if predictions[class_i] < recall_thresholds[class_name]:\n",
    "            image_labels_empty.add(f'{img}_{class_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/densenet201cloudy/densenet201.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_nonempty = set(submission.loc[~submission['EncodedPixels'].isnull(), 'Image_Label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(image_labels_empty.intersection(predictions_nonempty))} masks would be removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing masks\n",
    "submission.loc[submission['Image_Label'].isin(image_labels_empty), 'EncodedPixels'] = np.nan\n",
    "submission.to_csv('submission_segmentation_and_classifier.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work\n",
    "1. estimate distribution of classes in test set using the classifier. Then, if necessary and doable, modify val set accordingly,\n",
    "2. use the classifier with explainability technique [Gradient-weighted Class Activation Mapping](http://gradcam.cloudcv.org/) to generate a baseline, (please see [GradCAM: extracting masks from classifier](https://www.kaggle.com/samusram/gradcam-extracting-masks-from-classifier)),\n",
    "3. improve the classifier,\n",
    "4. use the classifier as backbone for UNet-like solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "22d16de5dc5140928a6c0d581b8e3a3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "417ad312018e44dfad8151c6be6e9331": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6ef1f8163f6040268ece907cf7b15b03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b3fa440860214293b4d3c9c04fadc9ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b93962a03ebf4c868a8b9f8e8a804dc0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6ef1f8163f6040268ece907cf7b15b03",
       "placeholder": "​",
       "style": "IPY_MODEL_22d16de5dc5140928a6c0d581b8e3a3b",
       "value": "0/|/| 0/? [00:00&lt;?, ?it/s]"
      }
     },
     "bd48f48b47df48af8d3ef36314ecdf91": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e67d77e01ab64050a9016af3b1502206",
        "IPY_MODEL_b93962a03ebf4c868a8b9f8e8a804dc0"
       ],
       "layout": "IPY_MODEL_417ad312018e44dfad8151c6be6e9331"
      }
     },
     "e0b948daaa3e4fcf96acfb58c17fc67d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e67d77e01ab64050a9016af3b1502206": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b3fa440860214293b4d3c9c04fadc9ac",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e0b948daaa3e4fcf96acfb58c17fc67d",
       "value": 1
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
